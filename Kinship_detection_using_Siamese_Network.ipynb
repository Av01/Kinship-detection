{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kinship detection using Siamese Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Av01/Kinship-detection/blob/master/Kinship_detection_using_Siamese_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ixTYIS7lIMxF",
        "colab_type": "code",
        "colab": {},
        "outputId": "b34a4107-c8ce-429e-9687-d42b49a25276"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__notebook_source__.ipynb\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xT7uHh7IT53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mv kaggle.json ../root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_rb2t5lIUNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 ../root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1llACoGUIUcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4af5173f-802a-457d-f046-3e3c2e8a0118"
      },
      "source": [
        "!kaggle competitions download -c recognizing-faces-in-the-wild"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train_relationships.csv to /content\n",
            "\r  0% 0.00/77.6k [00:00<?, ?B/s]\n",
            "100% 77.6k/77.6k [00:00<00:00, 49.7MB/s]\n",
            "Downloading test.zip to /content\n",
            " 73% 25.0M/34.1M [00:00<00:00, 88.3MB/s]\n",
            "100% 34.1M/34.1M [00:00<00:00, 86.2MB/s]\n",
            "Downloading train.zip to /content\n",
            " 92% 63.0M/68.6M [00:00<00:00, 94.9MB/s]\n",
            "100% 68.6M/68.6M [00:00<00:00, 100MB/s] \n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/156k [00:00<?, ?B/s]\n",
            "100% 156k/156k [00:00<00:00, 145MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg5foQoeImwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('train.zip','r') as f:\n",
        "  f.extractall()\n",
        "with ZipFile('test.zip','r') as f:\n",
        "  f.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQoLsMdXJV5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ../home/Model1 ../home/Model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbittg6sNORY",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "63468d24-472c-4900-b07e-2b58a2293918"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "!mv checkpoint ../home/Model1/checkpoint\n",
        "!mv model.ckpt.data-00000-of-00001 ../home/Model1/model.ckpt.data-00000-of-00001\n",
        "!mv model.ckpt.meta ../home/Model1/model.ckpt.meta\n",
        "!mv model.ckpt.index ../home/Model1/model.ckpt.index"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-69cf4480-0759-4156-8231-5ecab24b250a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-69cf4480-0759-4156-8231-5ecab24b250a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint to checkpoint\n",
            "Saving model.ckpt.data-00000-of-00001 to model.ckpt.data-00000-of-00001\n",
            "Saving model.ckpt.index to model.ckpt.index\n",
            "Saving model.ckpt.meta to model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFGLenfgOA1_",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "fb7c1e34-8bad-4743-e376-03792dcdee27"
      },
      "source": [
        "uploaded = files.upload()\n",
        "!mv checkpoint ../home/Model2/checkpoint\n",
        "!mv model.ckpt.data-00000-of-00001 ../home/Model2/model.ckpt.data-00000-of-00001\n",
        "!mv model.ckpt.meta ../home/Model2/model.ckpt.meta\n",
        "!mv model.ckpt.index ../home/Model2/model.ckpt.index"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c7f7884-a98b-43d7-a7fa-b2de6261e968\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3c7f7884-a98b-43d7-a7fa-b2de6261e968\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint to checkpoint\n",
            "Saving model.ckpt.data-00000-of-00001 to model.ckpt.data-00000-of-00001\n",
            "Saving model.ckpt.index to model.ckpt.index\n",
            "Saving model.ckpt.meta to model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "wYOFXV_dIMxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "import glob\n",
        "import os\n",
        "import tensorflow as tf\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hWmk0GCoIMx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"train_relationships.csv\")\n",
        "test_df = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aLyrIKavIMyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image(image):\n",
        "    normalized_img = np.mean(image,axis = 2,keepdims = True)/255\n",
        "    shifted_img = np.roll(normalized_img,1,axis = 1)\n",
        "    return normalized_img - shifted_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XNT5wTagIMya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def generate_batch(pl1,pl2,show_size = False):\n",
        "    p1 = np.copy(pl1)\n",
        "    p2 = np.copy(pl2)\n",
        "    positive_images = []\n",
        "    for each_p1,each_p2 in zip(p1,p2):  #takes minimum of both\n",
        "            images1 = [process_image(np.array(Image.open(f))) for f in glob.glob(each_p1 + \"/*.jpg\",recursive = True)]\n",
        "            images2 = [process_image(np.array(Image.open(f))) for f in glob.glob(each_p2 + \"/*.jpg\",recursive = True)]\n",
        "            entries = []\n",
        "            for image1,image2 in zip(images1,images2):\n",
        "                entry = [each_p1,each_p2,image1,image2,1]\n",
        "                entries.append(entry)\n",
        "            positive_images.extend(entries)\n",
        "        \n",
        "    negative_images = []\n",
        "    random.shuffle(p2)\n",
        "    for each_n1,each_n2 in zip(p1,p2):\n",
        "            f1 = each_n1.split('/')[0]\n",
        "            f2 = each_n2.split('/')[0]\n",
        "            if f1 == f2:\n",
        "#             print(f1,f2)\n",
        "                continue\n",
        "            images1 = [process_image(np.array(Image.open(f))) for f in glob.glob(each_n1 + \"/*.jpg\",recursive = True)]\n",
        "            images2 = [process_image(np.array(Image.open(f))) for f in glob.glob(each_n2 + \"/*.jpg\",recursive = True)]\n",
        "            entries = []\n",
        "            for image1,image2 in zip(images1,images2):\n",
        "                entry = [[each_n1,each_n2,image1,image2,0],[each_n2,each_n1,image2,image1,0]]\n",
        "                entries.extend(entry)\n",
        "            negative_images.extend(entries)\n",
        "    pi = len(positive_images)\n",
        "    ni = len(negative_images)\n",
        "    if ni > pi:\n",
        "        negative_images = random.sample(negative_images,pi) \n",
        "    if show_size:\n",
        "        print(\"Sizes: \",len(positive_images),', ',len(negative_images))\n",
        "    batch = negative_images + positive_images\n",
        "    random.shuffle(batch)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DHAkB3ymIMzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.keep_prob = tf.placeholder(dtype = tf.float32)\n",
        "        self.rate = 1 - self.keep_prob \n",
        "        with tf.device('device:GPU:0'):\n",
        "            self.X1 = tf.placeholder(dtype = tf.float32, shape = [None,224,224,1])\n",
        "#             self.X1_n = tf.math.divide(self.X1,255)\n",
        "\n",
        "            \n",
        "        #weights\n",
        "            self.w_c1 = tf.Variable(tf.random.normal([20,20,1,32],mean = 0, stddev = 0.01),name = 'conv_w1')\n",
        "            self.w_c2 = tf.Variable(tf.random.normal([10,10,32,64],mean = 0, stddev = 0.01),name = 'conv_w2')\n",
        "            self.w_c3 = tf.Variable(tf.random.normal([5,5,64,128],mean = 0, stddev = 0.01),name = 'conv_w3')\n",
        "\n",
        "            self.w_l = tf.Variable(tf.random.normal([1000,1],mean = 0, stddev = 0.01),name = 'last_w')\n",
        "            self.b_l = tf.Variable(tf.random.normal([1],mean = 0, stddev = 0.01),name = 'last_b')\n",
        "    \n",
        "        #left_side\n",
        "        \n",
        "            self.cl1 = tf.nn.conv2d(self.X1,self.w_c1,[1,2,2,1],\"SAME\")\n",
        "            self.maxpooll1 = tf.nn.max_pool(self.cl1,ksize = [1,5,5,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "            self.dropoutl1 = tf.nn.dropout(self.maxpooll1,rate = self.rate)\n",
        "            self.cl2 = tf.nn.conv2d(self.dropoutl1,self.w_c2,[1,2,2,1],\"SAME\")\n",
        "            self.maxpooll2 = tf.nn.max_pool(self.cl2,ksize = [1,5,5,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "            self.dropoutl2 = tf.nn.dropout(self.maxpooll2,rate = self.rate) \n",
        "            self.cl3 = tf.nn.conv2d(self.dropoutl2,self.w_c3,[1,2,2,1],\"SAME\")\n",
        "            self.maxpooll3 = tf.nn.max_pool(self.cl3,ksize = [1,5,5,1],strides=[1,2,2,1],padding=\"SAME\")\n",
        "            self.dropoutl3 = tf.nn.dropout(self.maxpooll3,rate = self.rate)\n",
        "            d = self.dropoutl3.shape\n",
        "        \n",
        "            self.w_f = tf.Variable(tf.random.normal([int(d[1] * d[2] * d[3]), 1000],mean = 0, stddev = 0.01),name = 'fully_w')\n",
        "            self.b_f = tf.Variable(tf.random.normal([1000],mean = 0, stddev = 0.01),name = 'fully_b')\n",
        "        \n",
        "            self.flatl1 = tf.reshape(self.dropoutl3, [-1,d[1] * d[2] * d[3]])\n",
        "            self.l1 = tf.nn.relu(tf.matmul(self.flatl1,self.w_f) + self.b_f)\n",
        "            print(self.l1.shape)\n",
        "\n",
        "        \n",
        "            self.l2 = tf.placeholder(dtype = tf.float32, shape = [None,1000])\n",
        "            self.l = tf.math.abs(self.l1 - self.l2)\n",
        "            self.out = tf.matmul(self.l,self.w_l) + self.b_l\n",
        "            self.output = tf.math.sigmoid(100 * self.out)\n",
        "            self.labels = tf.placeholder(dtype = tf.float32, shape = [None,1])\n",
        "            self.loss = self.calculate_loss(self.output,self.labels)\n",
        "            self.optimizer = self.optimize(self.loss)\n",
        "            self.init = tf.global_variables_initializer()\n",
        "        self.saver = tf.train.Saver([self.w_c1,self.w_c2,self.w_c3,self.w_f,self.b_f,self.w_l,self.b_l])\n",
        "        \n",
        "            \n",
        "    def calculate_loss(self,out,label):\n",
        "        return -tf.math.reduce_mean((label) * tf.math.log(out + 0.0001) + (1 - label) * tf.math.log(1 - out + 0.0001)) \n",
        "        \n",
        "    def optimize(self,loss):\n",
        "        return tf.train.RMSPropOptimizer(learning_rate = 0.0001).minimize(loss)\n",
        "    \n",
        "    def find_output(self,sess,X1,l2,keep_prob):\n",
        "        output = sess.run(self.output,feed_dict = {self.keep_prob: keep_prob, self.X1 : X1, self.l2 : l2})\n",
        "        return output\n",
        "    \n",
        "    def update_weights(self,sess,X1,l2,labels,keep_prob):\n",
        "        loss,_ = sess.run([self.loss,self.optimizer],feed_dict = {self.keep_prob: keep_prob, self.X1 : X1, self.l2 : l2, self.labels : labels})\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GiQ868FHIMzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_batch(batch):\n",
        "    X1 = []\n",
        "    X2 = []\n",
        "    labels = []\n",
        "    for each_entry in batch:\n",
        "        X1.append(each_entry[2])\n",
        "        X2.append(each_entry[3])\n",
        "        labels.append([each_entry[4]])\n",
        "    return np.array(X1),np.array(X2),np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rlr4oBSnIMzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "epoch = 100\n",
        "batch_n = 32\n",
        "train_data_n = train_df.shape[0]\n",
        "train_n = int(0.98 * train_data_n)\n",
        "test_n = train_data_n - train_n\n",
        "train_df = train_df.sample(frac = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UWp_XWbiIMzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47702
        },
        "outputId": "e908c1fb-1fe1-4ee3-d43d-1bc2e0c62fe0"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "model1 = Model()\n",
        "model2 = Model()\n",
        "Loss = []\n",
        "with tf.Session() as sess:\n",
        "    sess.run(model1.init)\n",
        "    sess.run(model2.init)\n",
        "    if os.path.exists(\"../home/Model1/checkpoint\"):\n",
        "        print(\"Restoring...\")\n",
        "        model1.saver.restore(sess,\"../home/Model2/model.ckpt\")\n",
        "        model2.saver.restore(sess,\"../home/Model1/model.ckpt\")\n",
        "    for e in range(epoch):\n",
        "        print(\"Starting epoch: \", e + 1)\n",
        "        Tloss = 0\n",
        "        for i in range(0,train_n,batch_n):\n",
        "            p1 = train_df['p1'][i:i + batch_n]\n",
        "            p2 = train_df['p2'][i:i + batch_n]\n",
        "            batch = generate_batch(p1,p2)\n",
        "            X1,X2,labels = process_batch(batch)\n",
        "            l2 = sess.run([model2.l1],feed_dict = {model2.keep_prob: 0.8, model2.X1 : X2})[0]\n",
        "            output = model1.find_output(sess,X1,l2,0.8)\n",
        "            loss = model1.update_weights(sess,X1,l2,labels,0.8)\n",
        "#             out = sess.run([model1.out],feed_dict = {model1.X1 : X1, model1.l2 : l2})\n",
        "#             print(out[:3])\n",
        "#             output = sess.run([model.output],feed_dict = {model.X1 : X1, model.X2 : X2})\n",
        "#             loss,_ = sess.run([model.loss,model.optimizer],feed_dict = {model.X1 : X1, model.X2 : X2, model.labels : labels})\n",
        "#             print(sess.run([model.output[:10],model.w_c1[:2,:2,0,0]],feed_dict = {model.X1 : X1, model.X2 : X2, model.labels : labels}))\n",
        "            Tloss += loss\n",
        "            batch_accuracy = np.mean(np.equal(labels,np.round(output)))\n",
        "            if i % (10 * batch_n) == 0:\n",
        "                print(\"Batch accuracy: \", batch_accuracy)\n",
        "                print(\"Batch loss: \",loss)\n",
        "                model1.saver.save(sess,\"../home/Model1/model.ckpt\")\n",
        "                model2.saver.save(sess,\"../home/Model2/model.ckpt\")\n",
        "            model1,model2 = model2, model1\n",
        "        print(\"Epoch ended\")\n",
        "        print(\"Total loss: \",Tloss)\n",
        "        print(\"Average batch loss: \",(Tloss / (train_n//batch_n)))\n",
        "        Loss.append(Tloss)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1000)\n",
            "(?, 1000)\n",
            "Restoring...\n",
            "INFO:tensorflow:Restoring parameters from ../home/Model2/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from ../home/Model1/model.ckpt\n",
            "Starting epoch:  1\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.3582245\n",
            "Batch accuracy:  0.7564102564102564\n",
            "Batch loss:  0.4533959\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.34505785\n",
            "Batch accuracy:  0.8412162162162162\n",
            "Batch loss:  0.3940743\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.40475935\n",
            "Batch accuracy:  0.7978723404255319\n",
            "Batch loss:  0.46287894\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.41565832\n",
            "Batch accuracy:  0.8252427184466019\n",
            "Batch loss:  0.35722604\n",
            "Batch accuracy:  0.7653061224489796\n",
            "Batch loss:  0.46552214\n",
            "Batch accuracy:  0.8227272727272728\n",
            "Batch loss:  0.38691306\n",
            "Batch accuracy:  0.7815126050420168\n",
            "Batch loss:  0.42651317\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.40495446\n",
            "Epoch ended\n",
            "Total loss:  43.33763003349304\n",
            "Average batch loss:  0.39397845484993677\n",
            "Starting epoch:  2\n",
            "Batch accuracy:  0.8089430894308943\n",
            "Batch loss:  0.3720278\n",
            "Batch accuracy:  0.7948717948717948\n",
            "Batch loss:  0.44026223\n",
            "Batch accuracy:  0.8108108108108109\n",
            "Batch loss:  0.44299594\n",
            "Batch accuracy:  0.8175675675675675\n",
            "Batch loss:  0.40836006\n",
            "Batch accuracy:  0.7962962962962963\n",
            "Batch loss:  0.4617991\n",
            "Batch accuracy:  0.8297872340425532\n",
            "Batch loss:  0.36567977\n",
            "Batch accuracy:  0.7991071428571429\n",
            "Batch loss:  0.49348858\n",
            "Batch accuracy:  0.8106796116504854\n",
            "Batch loss:  0.3690094\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.4049479\n",
            "Batch accuracy:  0.7181818181818181\n",
            "Batch loss:  0.505027\n",
            "Batch accuracy:  0.819327731092437\n",
            "Batch loss:  0.44710836\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.3957889\n",
            "Epoch ended\n",
            "Total loss:  45.62248858809471\n",
            "Average batch loss:  0.4147498962554065\n",
            "Starting epoch:  3\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.38110584\n",
            "Batch accuracy:  0.8076923076923077\n",
            "Batch loss:  0.434088\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.39476508\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.4034727\n",
            "Batch accuracy:  0.8564814814814815\n",
            "Batch loss:  0.3547586\n",
            "Batch accuracy:  0.7819148936170213\n",
            "Batch loss:  0.52244407\n",
            "Batch accuracy:  0.84375\n",
            "Batch loss:  0.36867052\n",
            "Batch accuracy:  0.8252427184466019\n",
            "Batch loss:  0.3527182\n",
            "Batch accuracy:  0.7755102040816326\n",
            "Batch loss:  0.47904742\n",
            "Batch accuracy:  0.7909090909090909\n",
            "Batch loss:  0.5025492\n",
            "Batch accuracy:  0.8067226890756303\n",
            "Batch loss:  0.43849275\n",
            "Batch accuracy:  0.7558139534883721\n",
            "Batch loss:  0.48113286\n",
            "Epoch ended\n",
            "Total loss:  45.884465366601944\n",
            "Average batch loss:  0.41713150333274496\n",
            "Starting epoch:  4\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.39238143\n",
            "Batch accuracy:  0.8076923076923077\n",
            "Batch loss:  0.44621652\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.34488973\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.39275768\n",
            "Batch accuracy:  0.7731481481481481\n",
            "Batch loss:  0.4136109\n",
            "Batch accuracy:  0.8457446808510638\n",
            "Batch loss:  0.42859164\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.4776903\n",
            "Batch accuracy:  0.8300970873786407\n",
            "Batch loss:  0.34866858\n",
            "Batch accuracy:  0.8112244897959183\n",
            "Batch loss:  0.37595788\n",
            "Batch accuracy:  0.75\n",
            "Batch loss:  0.5110396\n",
            "Batch accuracy:  0.8529411764705882\n",
            "Batch loss:  0.38546968\n",
            "Batch accuracy:  0.7248062015503876\n",
            "Batch loss:  0.58753055\n",
            "Epoch ended\n",
            "Total loss:  48.271947383880615\n",
            "Average batch loss:  0.4388358853080056\n",
            "Starting epoch:  5\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3961836\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.43456864\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.3641734\n",
            "Batch accuracy:  0.8040540540540541\n",
            "Batch loss:  0.48764086\n",
            "Batch accuracy:  0.8287037037037037\n",
            "Batch loss:  0.43774056\n",
            "Batch accuracy:  0.75\n",
            "Batch loss:  0.44332504\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.39402962\n",
            "Batch accuracy:  0.8106796116504854\n",
            "Batch loss:  0.3865738\n",
            "Batch accuracy:  0.8112244897959183\n",
            "Batch loss:  0.46801442\n",
            "Batch accuracy:  0.7363636363636363\n",
            "Batch loss:  0.4677871\n",
            "Batch accuracy:  0.8025210084033614\n",
            "Batch loss:  0.3702011\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.40951464\n",
            "Epoch ended\n",
            "Total loss:  46.0185544192791\n",
            "Average batch loss:  0.41835049472071906\n",
            "Starting epoch:  6\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.39651728\n",
            "Batch accuracy:  0.8076923076923077\n",
            "Batch loss:  0.3819852\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.49383587\n",
            "Batch accuracy:  0.7837837837837838\n",
            "Batch loss:  0.4431194\n",
            "Batch accuracy:  0.8009259259259259\n",
            "Batch loss:  0.3386023\n",
            "Batch accuracy:  0.8351063829787234\n",
            "Batch loss:  0.43288928\n",
            "Batch accuracy:  0.8125\n",
            "Batch loss:  0.4524735\n",
            "Batch accuracy:  0.8058252427184466\n",
            "Batch loss:  0.42179295\n",
            "Batch accuracy:  0.8316326530612245\n",
            "Batch loss:  0.33244225\n",
            "Batch accuracy:  0.75\n",
            "Batch loss:  0.5305341\n",
            "Batch accuracy:  0.726890756302521\n",
            "Batch loss:  0.5345686\n",
            "Batch accuracy:  0.7558139534883721\n",
            "Batch loss:  0.4885186\n",
            "Epoch ended\n",
            "Total loss:  45.79536488652229\n",
            "Average batch loss:  0.4163214989683845\n",
            "Starting epoch:  7\n",
            "Batch accuracy:  0.7886178861788617\n",
            "Batch loss:  0.44198683\n",
            "Batch accuracy:  0.8076923076923077\n",
            "Batch loss:  0.35557142\n",
            "Batch accuracy:  0.8040540540540541\n",
            "Batch loss:  0.3513907\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.39282218\n",
            "Batch accuracy:  0.8055555555555556\n",
            "Batch loss:  0.46465343\n",
            "Batch accuracy:  0.8031914893617021\n",
            "Batch loss:  0.45397243\n",
            "Batch accuracy:  0.7723214285714286\n",
            "Batch loss:  0.4502716\n",
            "Batch accuracy:  0.8106796116504854\n",
            "Batch loss:  0.38737482\n",
            "Batch accuracy:  0.8214285714285714\n",
            "Batch loss:  0.3796581\n",
            "Batch accuracy:  0.7681818181818182\n",
            "Batch loss:  0.50812715\n",
            "Batch accuracy:  0.7058823529411765\n",
            "Batch loss:  0.5367655\n",
            "Batch accuracy:  0.7984496124031008\n",
            "Batch loss:  0.5313972\n",
            "Epoch ended\n",
            "Total loss:  46.29442968964577\n",
            "Average batch loss:  0.42085845172405245\n",
            "Starting epoch:  8\n",
            "Batch accuracy:  0.7886178861788617\n",
            "Batch loss:  0.38474122\n",
            "Batch accuracy:  0.7564102564102564\n",
            "Batch loss:  0.42012167\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.3576307\n",
            "Batch accuracy:  0.7837837837837838\n",
            "Batch loss:  0.45590705\n",
            "Batch accuracy:  0.8055555555555556\n",
            "Batch loss:  0.4447948\n",
            "Batch accuracy:  0.8351063829787234\n",
            "Batch loss:  0.40299842\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.63546115\n",
            "Batch accuracy:  0.8106796116504854\n",
            "Batch loss:  0.41441503\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.3966895\n",
            "Batch accuracy:  0.7954545454545454\n",
            "Batch loss:  0.48898435\n",
            "Batch accuracy:  0.7815126050420168\n",
            "Batch loss:  0.42845383\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.34854084\n",
            "Epoch ended\n",
            "Total loss:  46.485437005758286\n",
            "Average batch loss:  0.42259488187052985\n",
            "Starting epoch:  9\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.43085372\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3542244\n",
            "Batch accuracy:  0.7905405405405406\n",
            "Batch loss:  0.3658165\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.33064362\n",
            "Batch accuracy:  0.8101851851851852\n",
            "Batch loss:  0.52252525\n",
            "Batch accuracy:  0.8031914893617021\n",
            "Batch loss:  0.4415961\n",
            "Batch accuracy:  0.8616071428571429\n",
            "Batch loss:  0.3473293\n",
            "Batch accuracy:  0.8398058252427184\n",
            "Batch loss:  0.35624486\n",
            "Batch accuracy:  0.7959183673469388\n",
            "Batch loss:  0.48086858\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.4776485\n",
            "Batch accuracy:  0.8445378151260504\n",
            "Batch loss:  0.39737886\n",
            "Batch accuracy:  0.6976744186046512\n",
            "Batch loss:  0.516569\n",
            "Epoch ended\n",
            "Total loss:  45.649756610393524\n",
            "Average batch loss:  0.4149977873672139\n",
            "Starting epoch:  10\n",
            "Batch accuracy:  0.8414634146341463\n",
            "Batch loss:  0.39199418\n",
            "Batch accuracy:  0.8205128205128205\n",
            "Batch loss:  0.4700109\n",
            "Batch accuracy:  0.8581081081081081\n",
            "Batch loss:  0.38514215\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.42637053\n",
            "Batch accuracy:  0.8518518518518519\n",
            "Batch loss:  0.34494075\n",
            "Batch accuracy:  0.7978723404255319\n",
            "Batch loss:  0.5107583\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.46236083\n",
            "Batch accuracy:  0.7766990291262136\n",
            "Batch loss:  0.35896528\n",
            "Batch accuracy:  0.8112244897959183\n",
            "Batch loss:  0.37055197\n",
            "Batch accuracy:  0.7272727272727273\n",
            "Batch loss:  0.5284955\n",
            "Batch accuracy:  0.7647058823529411\n",
            "Batch loss:  0.46276557\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.51245165\n",
            "Epoch ended\n",
            "Total loss:  46.45288294553757\n",
            "Average batch loss:  0.42229893586852335\n",
            "Starting epoch:  11\n",
            "Batch accuracy:  0.8414634146341463\n",
            "Batch loss:  0.39672318\n",
            "Batch accuracy:  0.7628205128205128\n",
            "Batch loss:  0.46651712\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.3269928\n",
            "Batch accuracy:  0.7871621621621622\n",
            "Batch loss:  0.36469483\n",
            "Batch accuracy:  0.8009259259259259\n",
            "Batch loss:  0.45729637\n",
            "Batch accuracy:  0.824468085106383\n",
            "Batch loss:  0.412181\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.63917524\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.30047047\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.5374225\n",
            "Batch accuracy:  0.8\n",
            "Batch loss:  0.4505533\n",
            "Batch accuracy:  0.7941176470588235\n",
            "Batch loss:  0.44968805\n",
            "Batch accuracy:  0.872093023255814\n",
            "Batch loss:  0.36613372\n",
            "Epoch ended\n",
            "Total loss:  45.79687389731407\n",
            "Average batch loss:  0.41633521724830974\n",
            "Starting epoch:  12\n",
            "Batch accuracy:  0.8211382113821138\n",
            "Batch loss:  0.3958151\n",
            "Batch accuracy:  0.8076923076923077\n",
            "Batch loss:  0.3874179\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.31840736\n",
            "Batch accuracy:  0.7972972972972973\n",
            "Batch loss:  0.48442855\n",
            "Batch accuracy:  0.7546296296296297\n",
            "Batch loss:  0.55183136\n",
            "Batch accuracy:  0.7712765957446809\n",
            "Batch loss:  0.45424706\n",
            "Batch accuracy:  0.8125\n",
            "Batch loss:  0.38465884\n",
            "Batch accuracy:  0.7961165048543689\n",
            "Batch loss:  0.4488487\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.40393895\n",
            "Batch accuracy:  0.7909090909090909\n",
            "Batch loss:  0.3861584\n",
            "Batch accuracy:  0.8403361344537815\n",
            "Batch loss:  0.45224246\n",
            "Batch accuracy:  0.7790697674418605\n",
            "Batch loss:  0.45284936\n",
            "Epoch ended\n",
            "Total loss:  45.235951751470566\n",
            "Average batch loss:  0.41123592501336875\n",
            "Starting epoch:  13\n",
            "Batch accuracy:  0.8699186991869918\n",
            "Batch loss:  0.36112568\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.36537012\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.37507144\n",
            "Batch accuracy:  0.8209459459459459\n",
            "Batch loss:  0.36554956\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.3071951\n",
            "Batch accuracy:  0.8191489361702128\n",
            "Batch loss:  0.3855425\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.5018404\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.3335912\n",
            "Batch accuracy:  0.7959183673469388\n",
            "Batch loss:  0.38250703\n",
            "Batch accuracy:  0.8590909090909091\n",
            "Batch loss:  0.39926308\n",
            "Batch accuracy:  0.8109243697478992\n",
            "Batch loss:  0.45867163\n",
            "Batch accuracy:  0.7906976744186046\n",
            "Batch loss:  0.49125358\n",
            "Epoch ended\n",
            "Total loss:  44.69984170794487\n",
            "Average batch loss:  0.4063621973449534\n",
            "Starting epoch:  14\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.30384228\n",
            "Batch accuracy:  0.782051282051282\n",
            "Batch loss:  0.48690888\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.37261185\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.4306291\n",
            "Batch accuracy:  0.7870370370370371\n",
            "Batch loss:  0.42412287\n",
            "Batch accuracy:  0.8297872340425532\n",
            "Batch loss:  0.44649774\n",
            "Batch accuracy:  0.78125\n",
            "Batch loss:  0.43394566\n",
            "Batch accuracy:  0.8300970873786407\n",
            "Batch loss:  0.3798554\n",
            "Batch accuracy:  0.8316326530612245\n",
            "Batch loss:  0.40839186\n",
            "Batch accuracy:  0.8181818181818182\n",
            "Batch loss:  0.4535633\n",
            "Batch accuracy:  0.7647058823529411\n",
            "Batch loss:  0.45035988\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.422769\n",
            "Epoch ended\n",
            "Total loss:  44.05582572519779\n",
            "Average batch loss:  0.4005075065927072\n",
            "Starting epoch:  15\n",
            "Batch accuracy:  0.8699186991869918\n",
            "Batch loss:  0.30459565\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.43172577\n",
            "Batch accuracy:  0.7972972972972973\n",
            "Batch loss:  0.477563\n",
            "Batch accuracy:  0.7804054054054054\n",
            "Batch loss:  0.46045798\n",
            "Batch accuracy:  0.8287037037037037\n",
            "Batch loss:  0.45525357\n",
            "Batch accuracy:  0.824468085106383\n",
            "Batch loss:  0.41942024\n",
            "Batch accuracy:  0.8080357142857143\n",
            "Batch loss:  0.4365638\n",
            "Batch accuracy:  0.8300970873786407\n",
            "Batch loss:  0.37676886\n",
            "Batch accuracy:  0.8622448979591837\n",
            "Batch loss:  0.30766064\n",
            "Batch accuracy:  0.7318181818181818\n",
            "Batch loss:  0.58520454\n",
            "Batch accuracy:  0.8025210084033614\n",
            "Batch loss:  0.4035636\n",
            "Batch accuracy:  0.810077519379845\n",
            "Batch loss:  0.441248\n",
            "Epoch ended\n",
            "Total loss:  44.706962540745735\n",
            "Average batch loss:  0.4064269321885976\n",
            "Starting epoch:  16\n",
            "Batch accuracy:  0.8008130081300813\n",
            "Batch loss:  0.44352096\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.44517386\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.36177593\n",
            "Batch accuracy:  0.8243243243243243\n",
            "Batch loss:  0.4050144\n",
            "Batch accuracy:  0.8240740740740741\n",
            "Batch loss:  0.41419408\n",
            "Batch accuracy:  0.8297872340425532\n",
            "Batch loss:  0.47918868\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.5003634\n",
            "Batch accuracy:  0.8398058252427184\n",
            "Batch loss:  0.33135974\n",
            "Batch accuracy:  0.8418367346938775\n",
            "Batch loss:  0.35592502\n",
            "Batch accuracy:  0.7727272727272727\n",
            "Batch loss:  0.46264517\n",
            "Batch accuracy:  0.8277310924369747\n",
            "Batch loss:  0.43802813\n",
            "Batch accuracy:  0.8062015503875969\n",
            "Batch loss:  0.44977063\n",
            "Epoch ended\n",
            "Total loss:  44.51568824052811\n",
            "Average batch loss:  0.40468807491389186\n",
            "Starting epoch:  17\n",
            "Batch accuracy:  0.7845528455284553\n",
            "Batch loss:  0.43612626\n",
            "Batch accuracy:  0.7692307692307693\n",
            "Batch loss:  0.42038733\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.37179974\n",
            "Batch accuracy:  0.8581081081081081\n",
            "Batch loss:  0.3586653\n",
            "Batch accuracy:  0.8564814814814815\n",
            "Batch loss:  0.34846044\n",
            "Batch accuracy:  0.75\n",
            "Batch loss:  0.48576722\n",
            "Batch accuracy:  0.8125\n",
            "Batch loss:  0.42243746\n",
            "Batch accuracy:  0.8058252427184466\n",
            "Batch loss:  0.41723317\n",
            "Batch accuracy:  0.826530612244898\n",
            "Batch loss:  0.37044656\n",
            "Batch accuracy:  0.740909090909091\n",
            "Batch loss:  0.531057\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.43557015\n",
            "Batch accuracy:  0.8023255813953488\n",
            "Batch loss:  0.40400922\n",
            "Epoch ended\n",
            "Total loss:  43.63898530602455\n",
            "Average batch loss:  0.39671804823658685\n",
            "Starting epoch:  18\n",
            "Batch accuracy:  0.8211382113821138\n",
            "Batch loss:  0.41796717\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.3648122\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.37359515\n",
            "Batch accuracy:  0.8074324324324325\n",
            "Batch loss:  0.4323618\n",
            "Batch accuracy:  0.7824074074074074\n",
            "Batch loss:  0.51207876\n",
            "Batch accuracy:  0.7925531914893617\n",
            "Batch loss:  0.40559274\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.40707713\n",
            "Batch accuracy:  0.8106796116504854\n",
            "Batch loss:  0.39880162\n",
            "Batch accuracy:  0.7755102040816326\n",
            "Batch loss:  0.40675828\n",
            "Batch accuracy:  0.759090909090909\n",
            "Batch loss:  0.49276185\n",
            "Batch accuracy:  0.8277310924369747\n",
            "Batch loss:  0.4090033\n",
            "Batch accuracy:  0.7713178294573644\n",
            "Batch loss:  0.545654\n",
            "Epoch ended\n",
            "Total loss:  43.81196504831314\n",
            "Average batch loss:  0.3982905913483013\n",
            "Starting epoch:  19\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.3712663\n",
            "Batch accuracy:  0.8782051282051282\n",
            "Batch loss:  0.3472217\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.30661687\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.35001367\n",
            "Batch accuracy:  0.8240740740740741\n",
            "Batch loss:  0.33383256\n",
            "Batch accuracy:  0.8404255319148937\n",
            "Batch loss:  0.4367614\n",
            "Batch accuracy:  0.78125\n",
            "Batch loss:  0.541513\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.35347173\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.39711782\n",
            "Batch accuracy:  0.7863636363636364\n",
            "Batch loss:  0.4405602\n",
            "Batch accuracy:  0.8067226890756303\n",
            "Batch loss:  0.4623969\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.3989721\n",
            "Epoch ended\n",
            "Total loss:  43.0675253868103\n",
            "Average batch loss:  0.39152295806191184\n",
            "Starting epoch:  20\n",
            "Batch accuracy:  0.7967479674796748\n",
            "Batch loss:  0.3966333\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.40035453\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.32979384\n",
            "Batch accuracy:  0.8040540540540541\n",
            "Batch loss:  0.41520566\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.25899255\n",
            "Batch accuracy:  0.8138297872340425\n",
            "Batch loss:  0.48995638\n",
            "Batch accuracy:  0.7544642857142857\n",
            "Batch loss:  0.47161993\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.3272881\n",
            "Batch accuracy:  0.8928571428571429\n",
            "Batch loss:  0.36967394\n",
            "Batch accuracy:  0.8363636363636363\n",
            "Batch loss:  0.391884\n",
            "Batch accuracy:  0.8109243697478992\n",
            "Batch loss:  0.4051974\n",
            "Batch accuracy:  0.810077519379845\n",
            "Batch loss:  0.42678967\n",
            "Epoch ended\n",
            "Total loss:  43.69488260149956\n",
            "Average batch loss:  0.3972262054681778\n",
            "Starting epoch:  21\n",
            "Batch accuracy:  0.8089430894308943\n",
            "Batch loss:  0.38115028\n",
            "Batch accuracy:  0.8589743589743589\n",
            "Batch loss:  0.4244738\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.3213334\n",
            "Batch accuracy:  0.8209459459459459\n",
            "Batch loss:  0.449306\n",
            "Batch accuracy:  0.8564814814814815\n",
            "Batch loss:  0.35439828\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.39819953\n",
            "Batch accuracy:  0.8794642857142857\n",
            "Batch loss:  0.31329292\n",
            "Batch accuracy:  0.8640776699029126\n",
            "Batch loss:  0.251661\n",
            "Batch accuracy:  0.7959183673469388\n",
            "Batch loss:  0.48258066\n",
            "Batch accuracy:  0.8045454545454546\n",
            "Batch loss:  0.47494605\n",
            "Batch accuracy:  0.8235294117647058\n",
            "Batch loss:  0.4153384\n",
            "Batch accuracy:  0.7906976744186046\n",
            "Batch loss:  0.48606062\n",
            "Epoch ended\n",
            "Total loss:  43.8937828540802\n",
            "Average batch loss:  0.3990343895825473\n",
            "Starting epoch:  22\n",
            "Batch accuracy:  0.8252032520325203\n",
            "Batch loss:  0.43376777\n",
            "Batch accuracy:  0.8141025641025641\n",
            "Batch loss:  0.43802613\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.32107726\n",
            "Batch accuracy:  0.7837837837837838\n",
            "Batch loss:  0.5684019\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.3780863\n",
            "Batch accuracy:  0.8297872340425532\n",
            "Batch loss:  0.3745724\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.46180576\n",
            "Batch accuracy:  0.8349514563106796\n",
            "Batch loss:  0.3084868\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.31968978\n",
            "Batch accuracy:  0.8272727272727273\n",
            "Batch loss:  0.3736722\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.33886814\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.43110028\n",
            "Epoch ended\n",
            "Total loss:  43.26380789279938\n",
            "Average batch loss:  0.3933073444799943\n",
            "Starting epoch:  23\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.376755\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.31649765\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.32648003\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.41517252\n",
            "Batch accuracy:  0.8101851851851852\n",
            "Batch loss:  0.34235054\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.32127798\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.37815493\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.30928943\n",
            "Batch accuracy:  0.8214285714285714\n",
            "Batch loss:  0.44426152\n",
            "Batch accuracy:  0.8090909090909091\n",
            "Batch loss:  0.42036423\n",
            "Batch accuracy:  0.8067226890756303\n",
            "Batch loss:  0.35191938\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.41134873\n",
            "Epoch ended\n",
            "Total loss:  42.39492008090019\n",
            "Average batch loss:  0.3854083643718199\n",
            "Starting epoch:  24\n",
            "Batch accuracy:  0.8211382113821138\n",
            "Batch loss:  0.41623768\n",
            "Batch accuracy:  0.8589743589743589\n",
            "Batch loss:  0.38779193\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.2917992\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.43731815\n",
            "Batch accuracy:  0.8194444444444444\n",
            "Batch loss:  0.41767198\n",
            "Batch accuracy:  0.7606382978723404\n",
            "Batch loss:  0.55621094\n",
            "Batch accuracy:  0.8169642857142857\n",
            "Batch loss:  0.39098778\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.292764\n",
            "Batch accuracy:  0.826530612244898\n",
            "Batch loss:  0.43347004\n",
            "Batch accuracy:  0.8045454545454546\n",
            "Batch loss:  0.4506851\n",
            "Batch accuracy:  0.8613445378151261\n",
            "Batch loss:  0.37314716\n",
            "Batch accuracy:  0.7790697674418605\n",
            "Batch loss:  0.44047275\n",
            "Epoch ended\n",
            "Total loss:  41.74675536155701\n",
            "Average batch loss:  0.37951595783233644\n",
            "Starting epoch:  25\n",
            "Batch accuracy:  0.7682926829268293\n",
            "Batch loss:  0.4948003\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.35628593\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.39593726\n",
            "Batch accuracy:  0.7668918918918919\n",
            "Batch loss:  0.4493823\n",
            "Batch accuracy:  0.8240740740740741\n",
            "Batch loss:  0.4131702\n",
            "Batch accuracy:  0.9042553191489362\n",
            "Batch loss:  0.2962388\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.36734468\n",
            "Batch accuracy:  0.8446601941747572\n",
            "Batch loss:  0.35138914\n",
            "Batch accuracy:  0.7806122448979592\n",
            "Batch loss:  0.45447096\n",
            "Batch accuracy:  0.8090909090909091\n",
            "Batch loss:  0.43207443\n",
            "Batch accuracy:  0.8403361344537815\n",
            "Batch loss:  0.45605877\n",
            "Batch accuracy:  0.7558139534883721\n",
            "Batch loss:  0.495292\n",
            "Epoch ended\n",
            "Total loss:  43.01964509487152\n",
            "Average batch loss:  0.3910876826806502\n",
            "Starting epoch:  26\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3780654\n",
            "Batch accuracy:  0.8012820512820513\n",
            "Batch loss:  0.40274727\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.33388492\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.38301304\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.31735483\n",
            "Batch accuracy:  0.8617021276595744\n",
            "Batch loss:  0.30766433\n",
            "Batch accuracy:  0.7991071428571429\n",
            "Batch loss:  0.49064296\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.3556412\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.34746018\n",
            "Batch accuracy:  0.7909090909090909\n",
            "Batch loss:  0.4946709\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.40153807\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.39431712\n",
            "Epoch ended\n",
            "Total loss:  42.968045592308044\n",
            "Average batch loss:  0.3906185962937095\n",
            "Starting epoch:  27\n",
            "Batch accuracy:  0.8252032520325203\n",
            "Batch loss:  0.36140454\n",
            "Batch accuracy:  0.8269230769230769\n",
            "Batch loss:  0.42366883\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.3412171\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.28243214\n",
            "Batch accuracy:  0.8518518518518519\n",
            "Batch loss:  0.38263214\n",
            "Batch accuracy:  0.8031914893617021\n",
            "Batch loss:  0.49853894\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.35696623\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.28474966\n",
            "Batch accuracy:  0.826530612244898\n",
            "Batch loss:  0.4957324\n",
            "Batch accuracy:  0.8681818181818182\n",
            "Batch loss:  0.4002858\n",
            "Batch accuracy:  0.7563025210084033\n",
            "Batch loss:  0.59335375\n",
            "Batch accuracy:  0.8023255813953488\n",
            "Batch loss:  0.4155636\n",
            "Epoch ended\n",
            "Total loss:  41.24265715479851\n",
            "Average batch loss:  0.3749332468618046\n",
            "Starting epoch:  28\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.4142084\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.42307222\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.29053172\n",
            "Batch accuracy:  0.8006756756756757\n",
            "Batch loss:  0.46310085\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.35319313\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.31545806\n",
            "Batch accuracy:  0.8303571428571429\n",
            "Batch loss:  0.38399348\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.30245468\n",
            "Batch accuracy:  0.7857142857142857\n",
            "Batch loss:  0.42551118\n",
            "Batch accuracy:  0.7772727272727272\n",
            "Batch loss:  0.51878816\n",
            "Batch accuracy:  0.8865546218487395\n",
            "Batch loss:  0.28517967\n",
            "Batch accuracy:  0.8062015503875969\n",
            "Batch loss:  0.40559432\n",
            "Epoch ended\n",
            "Total loss:  42.21443721652031\n",
            "Average batch loss:  0.38376761105927554\n",
            "Starting epoch:  29\n",
            "Batch accuracy:  0.8170731707317073\n",
            "Batch loss:  0.40802926\n",
            "Batch accuracy:  0.8717948717948718\n",
            "Batch loss:  0.3727814\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.27946478\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.37153298\n",
            "Batch accuracy:  0.8518518518518519\n",
            "Batch loss:  0.3721937\n",
            "Batch accuracy:  0.8670212765957447\n",
            "Batch loss:  0.3686701\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.37951416\n",
            "Batch accuracy:  0.8592233009708737\n",
            "Batch loss:  0.3336739\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.37569633\n",
            "Batch accuracy:  0.7954545454545454\n",
            "Batch loss:  0.41861054\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.38805017\n",
            "Batch accuracy:  0.8449612403100775\n",
            "Batch loss:  0.3346545\n",
            "Epoch ended\n",
            "Total loss:  41.24317745864391\n",
            "Average batch loss:  0.37493797689676284\n",
            "Starting epoch:  30\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.36708122\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.48860344\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.2664191\n",
            "Batch accuracy:  0.8412162162162162\n",
            "Batch loss:  0.36749208\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.37256524\n",
            "Batch accuracy:  0.8191489361702128\n",
            "Batch loss:  0.39385125\n",
            "Batch accuracy:  0.7991071428571429\n",
            "Batch loss:  0.5023665\n",
            "Batch accuracy:  0.8689320388349514\n",
            "Batch loss:  0.3239331\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.28808886\n",
            "Batch accuracy:  0.7272727272727273\n",
            "Batch loss:  0.47683826\n",
            "Batch accuracy:  0.865546218487395\n",
            "Batch loss:  0.38109946\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.34937966\n",
            "Epoch ended\n",
            "Total loss:  42.02768933773041\n",
            "Average batch loss:  0.38206990307027644\n",
            "Starting epoch:  31\n",
            "Batch accuracy:  0.8414634146341463\n",
            "Batch loss:  0.3277748\n",
            "Batch accuracy:  0.7948717948717948\n",
            "Batch loss:  0.47747687\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.3072964\n",
            "Batch accuracy:  0.8074324324324325\n",
            "Batch loss:  0.38006753\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.38956487\n",
            "Batch accuracy:  0.8776595744680851\n",
            "Batch loss:  0.27075934\n",
            "Batch accuracy:  0.8258928571428571\n",
            "Batch loss:  0.31415397\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.3164126\n",
            "Batch accuracy:  0.8673469387755102\n",
            "Batch loss:  0.31557655\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.45603028\n",
            "Batch accuracy:  0.8403361344537815\n",
            "Batch loss:  0.3367236\n",
            "Batch accuracy:  0.8488372093023255\n",
            "Batch loss:  0.3372368\n",
            "Epoch ended\n",
            "Total loss:  40.64951950311661\n",
            "Average batch loss:  0.36954108639196914\n",
            "Starting epoch:  32\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.35340118\n",
            "Batch accuracy:  0.8269230769230769\n",
            "Batch loss:  0.40095466\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.26925147\n",
            "Batch accuracy:  0.7702702702702703\n",
            "Batch loss:  0.48761\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.3521864\n",
            "Batch accuracy:  0.8351063829787234\n",
            "Batch loss:  0.4665672\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.31866977\n",
            "Batch accuracy:  0.7815533980582524\n",
            "Batch loss:  0.36066937\n",
            "Batch accuracy:  0.8622448979591837\n",
            "Batch loss:  0.26959115\n",
            "Batch accuracy:  0.8272727272727273\n",
            "Batch loss:  0.43631452\n",
            "Batch accuracy:  0.8949579831932774\n",
            "Batch loss:  0.34107995\n",
            "Batch accuracy:  0.872093023255814\n",
            "Batch loss:  0.31430727\n",
            "Epoch ended\n",
            "Total loss:  41.655357867479324\n",
            "Average batch loss:  0.37868507152253933\n",
            "Starting epoch:  33\n",
            "Batch accuracy:  0.9065040650406504\n",
            "Batch loss:  0.24589898\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.36648253\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.25316048\n",
            "Batch accuracy:  0.8243243243243243\n",
            "Batch loss:  0.33572733\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.33193734\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.32897726\n",
            "Batch accuracy:  0.84375\n",
            "Batch loss:  0.42632124\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.35993353\n",
            "Batch accuracy:  0.8469387755102041\n",
            "Batch loss:  0.31384063\n",
            "Batch accuracy:  0.7090909090909091\n",
            "Batch loss:  0.619197\n",
            "Batch accuracy:  0.8277310924369747\n",
            "Batch loss:  0.3931846\n",
            "Batch accuracy:  0.810077519379845\n",
            "Batch loss:  0.45146257\n",
            "Epoch ended\n",
            "Total loss:  40.454953104257584\n",
            "Average batch loss:  0.3677723009477962\n",
            "Starting epoch:  34\n",
            "Batch accuracy:  0.8089430894308943\n",
            "Batch loss:  0.3969397\n",
            "Batch accuracy:  0.8141025641025641\n",
            "Batch loss:  0.40126672\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.2852005\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.473323\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.33702546\n",
            "Batch accuracy:  0.8829787234042553\n",
            "Batch loss:  0.32015252\n",
            "Batch accuracy:  0.8258928571428571\n",
            "Batch loss:  0.39650235\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.29553822\n",
            "Batch accuracy:  0.8418367346938775\n",
            "Batch loss:  0.3955599\n",
            "Batch accuracy:  0.8\n",
            "Batch loss:  0.4766673\n",
            "Batch accuracy:  0.8529411764705882\n",
            "Batch loss:  0.3704489\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.40288094\n",
            "Epoch ended\n",
            "Total loss:  40.788040563464165\n",
            "Average batch loss:  0.37080036875876515\n",
            "Starting epoch:  35\n",
            "Batch accuracy:  0.8373983739837398\n",
            "Batch loss:  0.4995067\n",
            "Batch accuracy:  0.7948717948717948\n",
            "Batch loss:  0.39594942\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.37157482\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.38950393\n",
            "Batch accuracy:  0.8194444444444444\n",
            "Batch loss:  0.38327053\n",
            "Batch accuracy:  0.8457446808510638\n",
            "Batch loss:  0.35301548\n",
            "Batch accuracy:  0.8080357142857143\n",
            "Batch loss:  0.36500123\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.3576425\n",
            "Batch accuracy:  0.8520408163265306\n",
            "Batch loss:  0.31795734\n",
            "Batch accuracy:  0.8181818181818182\n",
            "Batch loss:  0.41146255\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.32329243\n",
            "Batch accuracy:  0.8062015503875969\n",
            "Batch loss:  0.37701452\n",
            "Epoch ended\n",
            "Total loss:  41.48182499408722\n",
            "Average batch loss:  0.3771074999462474\n",
            "Starting epoch:  36\n",
            "Batch accuracy:  0.8617886178861789\n",
            "Batch loss:  0.33841935\n",
            "Batch accuracy:  0.8012820512820513\n",
            "Batch loss:  0.3921699\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.27494168\n",
            "Batch accuracy:  0.8108108108108109\n",
            "Batch loss:  0.33517542\n",
            "Batch accuracy:  0.8796296296296297\n",
            "Batch loss:  0.2671212\n",
            "Batch accuracy:  0.8031914893617021\n",
            "Batch loss:  0.4188787\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.40280136\n",
            "Batch accuracy:  0.8640776699029126\n",
            "Batch loss:  0.366109\n",
            "Batch accuracy:  0.8163265306122449\n",
            "Batch loss:  0.37725228\n",
            "Batch accuracy:  0.8090909090909091\n",
            "Batch loss:  0.3781185\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.37479913\n",
            "Batch accuracy:  0.875968992248062\n",
            "Batch loss:  0.35397893\n",
            "Epoch ended\n",
            "Total loss:  40.70002081990242\n",
            "Average batch loss:  0.3700001892718402\n",
            "Starting epoch:  37\n",
            "Batch accuracy:  0.8252032520325203\n",
            "Batch loss:  0.40572077\n",
            "Batch accuracy:  0.8782051282051282\n",
            "Batch loss:  0.27004975\n",
            "Batch accuracy:  0.8581081081081081\n",
            "Batch loss:  0.3656963\n",
            "Batch accuracy:  0.8243243243243243\n",
            "Batch loss:  0.37107828\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.3701831\n",
            "Batch accuracy:  0.824468085106383\n",
            "Batch loss:  0.359866\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.4203122\n",
            "Batch accuracy:  0.8592233009708737\n",
            "Batch loss:  0.2747137\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.2894494\n",
            "Batch accuracy:  0.8409090909090909\n",
            "Batch loss:  0.41433898\n",
            "Batch accuracy:  0.8445378151260504\n",
            "Batch loss:  0.34249356\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.4146547\n",
            "Epoch ended\n",
            "Total loss:  39.850223407149315\n",
            "Average batch loss:  0.36227475824681193\n",
            "Starting epoch:  38\n",
            "Batch accuracy:  0.8658536585365854\n",
            "Batch loss:  0.39008248\n",
            "Batch accuracy:  0.8269230769230769\n",
            "Batch loss:  0.3537251\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.3323978\n",
            "Batch accuracy:  0.8006756756756757\n",
            "Batch loss:  0.3852896\n",
            "Batch accuracy:  0.8842592592592593\n",
            "Batch loss:  0.32686558\n",
            "Batch accuracy:  0.8457446808510638\n",
            "Batch loss:  0.4195941\n",
            "Batch accuracy:  0.8839285714285714\n",
            "Batch loss:  0.30218264\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.26875934\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.34172657\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.41482994\n",
            "Batch accuracy:  0.8151260504201681\n",
            "Batch loss:  0.44165626\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.40146324\n",
            "Epoch ended\n",
            "Total loss:  41.624958246946335\n",
            "Average batch loss:  0.3784087113358758\n",
            "Starting epoch:  39\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.32319266\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.27853152\n",
            "Batch accuracy:  0.8243243243243243\n",
            "Batch loss:  0.32838365\n",
            "Batch accuracy:  0.847972972972973\n",
            "Batch loss:  0.34272733\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.32552704\n",
            "Batch accuracy:  0.8776595744680851\n",
            "Batch loss:  0.27106562\n",
            "Batch accuracy:  0.8169642857142857\n",
            "Batch loss:  0.4862555\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.22879472\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.40162468\n",
            "Batch accuracy:  0.8227272727272728\n",
            "Batch loss:  0.45041507\n",
            "Batch accuracy:  0.8781512605042017\n",
            "Batch loss:  0.30357823\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.4023906\n",
            "Epoch ended\n",
            "Total loss:  39.45994697511196\n",
            "Average batch loss:  0.358726790682836\n",
            "Starting epoch:  40\n",
            "Batch accuracy:  0.8861788617886179\n",
            "Batch loss:  0.31500214\n",
            "Batch accuracy:  0.8141025641025641\n",
            "Batch loss:  0.38095644\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.33230904\n",
            "Batch accuracy:  0.7905405405405406\n",
            "Batch loss:  0.4521014\n",
            "Batch accuracy:  0.8564814814814815\n",
            "Batch loss:  0.33694112\n",
            "Batch accuracy:  0.7872340425531915\n",
            "Batch loss:  0.43650803\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.32332388\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.3529665\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.2749777\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.33723578\n",
            "Batch accuracy:  0.8403361344537815\n",
            "Batch loss:  0.33377627\n",
            "Batch accuracy:  0.8294573643410853\n",
            "Batch loss:  0.35358268\n",
            "Epoch ended\n",
            "Total loss:  39.73423273861408\n",
            "Average batch loss:  0.3612202976237644\n",
            "Starting epoch:  41\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.35061148\n",
            "Batch accuracy:  0.8717948717948718\n",
            "Batch loss:  0.26329896\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.2655162\n",
            "Batch accuracy:  0.8108108108108109\n",
            "Batch loss:  0.42623758\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.34450838\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.31087363\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.36188895\n",
            "Batch accuracy:  0.8300970873786407\n",
            "Batch loss:  0.36459148\n",
            "Batch accuracy:  0.9132653061224489\n",
            "Batch loss:  0.21994554\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.3979717\n",
            "Batch accuracy:  0.819327731092437\n",
            "Batch loss:  0.31549007\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.43769705\n",
            "Epoch ended\n",
            "Total loss:  39.7086019217968\n",
            "Average batch loss:  0.36098729019815273\n",
            "Starting epoch:  42\n",
            "Batch accuracy:  0.8373983739837398\n",
            "Batch loss:  0.39686042\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.42844293\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.22949053\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.31592938\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.31434637\n",
            "Batch accuracy:  0.8670212765957447\n",
            "Batch loss:  0.33710343\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.3452178\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.25333476\n",
            "Batch accuracy:  0.8520408163265306\n",
            "Batch loss:  0.3392143\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.35958123\n",
            "Batch accuracy:  0.865546218487395\n",
            "Batch loss:  0.31784692\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.37411806\n",
            "Epoch ended\n",
            "Total loss:  38.80132406949997\n",
            "Average batch loss:  0.352739309722727\n",
            "Starting epoch:  43\n",
            "Batch accuracy:  0.8495934959349594\n",
            "Batch loss:  0.3154986\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.285368\n",
            "Batch accuracy:  0.7094594594594594\n",
            "Batch loss:  0.5000099\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.48631582\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.40051877\n",
            "Batch accuracy:  0.8882978723404256\n",
            "Batch loss:  0.30870917\n",
            "Batch accuracy:  0.8125\n",
            "Batch loss:  0.38009042\n",
            "Batch accuracy:  0.8446601941747572\n",
            "Batch loss:  0.27117142\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.47242516\n",
            "Batch accuracy:  0.8\n",
            "Batch loss:  0.41494828\n",
            "Batch accuracy:  0.8865546218487395\n",
            "Batch loss:  0.30520707\n",
            "Batch accuracy:  0.7674418604651163\n",
            "Batch loss:  0.47616732\n",
            "Epoch ended\n",
            "Total loss:  40.47338615357876\n",
            "Average batch loss:  0.36793987412344326\n",
            "Starting epoch:  44\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3692044\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.3428153\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.25718847\n",
            "Batch accuracy:  0.847972972972973\n",
            "Batch loss:  0.4401292\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.3792086\n",
            "Batch accuracy:  0.8404255319148937\n",
            "Batch loss:  0.33676463\n",
            "Batch accuracy:  0.8928571428571429\n",
            "Batch loss:  0.27292842\n",
            "Batch accuracy:  0.8592233009708737\n",
            "Batch loss:  0.30369067\n",
            "Batch accuracy:  0.8673469387755102\n",
            "Batch loss:  0.34568727\n",
            "Batch accuracy:  0.7863636363636364\n",
            "Batch loss:  0.43566504\n",
            "Batch accuracy:  0.8865546218487395\n",
            "Batch loss:  0.2647995\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.33807024\n",
            "Epoch ended\n",
            "Total loss:  38.95487470924854\n",
            "Average batch loss:  0.35413522462953223\n",
            "Starting epoch:  45\n",
            "Batch accuracy:  0.8943089430894309\n",
            "Batch loss:  0.30548075\n",
            "Batch accuracy:  0.8589743589743589\n",
            "Batch loss:  0.35068777\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.24735188\n",
            "Batch accuracy:  0.793918918918919\n",
            "Batch loss:  0.44251963\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.39399365\n",
            "Batch accuracy:  0.8617021276595744\n",
            "Batch loss:  0.36873916\n",
            "Batch accuracy:  0.8883928571428571\n",
            "Batch loss:  0.29934925\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.2827781\n",
            "Batch accuracy:  0.8877551020408163\n",
            "Batch loss:  0.2712196\n",
            "Batch accuracy:  0.8409090909090909\n",
            "Batch loss:  0.3618746\n",
            "Batch accuracy:  0.8319327731092437\n",
            "Batch loss:  0.46457648\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.34431937\n",
            "Epoch ended\n",
            "Total loss:  39.515763238072395\n",
            "Average batch loss:  0.3592342112552036\n",
            "Starting epoch:  46\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.26154563\n",
            "Batch accuracy:  0.8974358974358975\n",
            "Batch loss:  0.35428149\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.28179\n",
            "Batch accuracy:  0.8209459459459459\n",
            "Batch loss:  0.35088032\n",
            "Batch accuracy:  0.8287037037037037\n",
            "Batch loss:  0.30544308\n",
            "Batch accuracy:  0.898936170212766\n",
            "Batch loss:  0.30392352\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.49191135\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.32709047\n",
            "Batch accuracy:  0.8316326530612245\n",
            "Batch loss:  0.3633427\n",
            "Batch accuracy:  0.8272727272727273\n",
            "Batch loss:  0.3960073\n",
            "Batch accuracy:  0.8823529411764706\n",
            "Batch loss:  0.31763813\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.41101217\n",
            "Epoch ended\n",
            "Total loss:  38.39050090312958\n",
            "Average batch loss:  0.34900455366481437\n",
            "Starting epoch:  47\n",
            "Batch accuracy:  0.8373983739837398\n",
            "Batch loss:  0.31246373\n",
            "Batch accuracy:  0.8269230769230769\n",
            "Batch loss:  0.6034846\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.26845354\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.30657053\n",
            "Batch accuracy:  0.8101851851851852\n",
            "Batch loss:  0.37573782\n",
            "Batch accuracy:  0.8617021276595744\n",
            "Batch loss:  0.37224194\n",
            "Batch accuracy:  0.8214285714285714\n",
            "Batch loss:  0.4836194\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.3187184\n",
            "Batch accuracy:  0.8469387755102041\n",
            "Batch loss:  0.3490877\n",
            "Batch accuracy:  0.8272727272727273\n",
            "Batch loss:  0.45981222\n",
            "Batch accuracy:  0.7983193277310925\n",
            "Batch loss:  0.38008955\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.42087284\n",
            "Epoch ended\n",
            "Total loss:  39.39918565750122\n",
            "Average batch loss:  0.3581744150681929\n",
            "Starting epoch:  48\n",
            "Batch accuracy:  0.7723577235772358\n",
            "Batch loss:  0.41376472\n",
            "Batch accuracy:  0.8653846153846154\n",
            "Batch loss:  0.3452889\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.26090366\n",
            "Batch accuracy:  0.7736486486486487\n",
            "Batch loss:  0.45463195\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.33007362\n",
            "Batch accuracy:  0.8457446808510638\n",
            "Batch loss:  0.3901146\n",
            "Batch accuracy:  0.8883928571428571\n",
            "Batch loss:  0.30278727\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.29647428\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.26969704\n",
            "Batch accuracy:  0.8318181818181818\n",
            "Batch loss:  0.446725\n",
            "Batch accuracy:  0.7899159663865546\n",
            "Batch loss:  0.4306399\n",
            "Batch accuracy:  0.9031007751937985\n",
            "Batch loss:  0.27144754\n",
            "Epoch ended\n",
            "Total loss:  38.810339108109474\n",
            "Average batch loss:  0.352821264619177\n",
            "Starting epoch:  49\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.3365898\n",
            "Batch accuracy:  0.782051282051282\n",
            "Batch loss:  0.45717925\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.34095696\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.34258834\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.27248457\n",
            "Batch accuracy:  0.8297872340425532\n",
            "Batch loss:  0.33222362\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.43316048\n",
            "Batch accuracy:  0.8543689320388349\n",
            "Batch loss:  0.3189854\n",
            "Batch accuracy:  0.7908163265306123\n",
            "Batch loss:  0.37864548\n",
            "Batch accuracy:  0.7363636363636363\n",
            "Batch loss:  0.5479454\n",
            "Batch accuracy:  0.7983193277310925\n",
            "Batch loss:  0.4149683\n",
            "Batch accuracy:  0.810077519379845\n",
            "Batch loss:  0.48641518\n",
            "Epoch ended\n",
            "Total loss:  38.39050729572773\n",
            "Average batch loss:  0.349004611779343\n",
            "Starting epoch:  50\n",
            "Batch accuracy:  0.8252032520325203\n",
            "Batch loss:  0.34506133\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.32942837\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.21345899\n",
            "Batch accuracy:  0.8547297297297297\n",
            "Batch loss:  0.28916943\n",
            "Batch accuracy:  0.9120370370370371\n",
            "Batch loss:  0.27940336\n",
            "Batch accuracy:  0.8617021276595744\n",
            "Batch loss:  0.3094369\n",
            "Batch accuracy:  0.7366071428571429\n",
            "Batch loss:  0.5275004\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.26767978\n",
            "Batch accuracy:  0.8877551020408163\n",
            "Batch loss:  0.31581962\n",
            "Batch accuracy:  0.8\n",
            "Batch loss:  0.4321717\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.33829352\n",
            "Batch accuracy:  0.8488372093023255\n",
            "Batch loss:  0.34239247\n",
            "Epoch ended\n",
            "Total loss:  39.16924299299717\n",
            "Average batch loss:  0.3560840272090652\n",
            "Starting epoch:  51\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.3444882\n",
            "Batch accuracy:  0.8205128205128205\n",
            "Batch loss:  0.54049486\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.35870847\n",
            "Batch accuracy:  0.8547297297297297\n",
            "Batch loss:  0.38817567\n",
            "Batch accuracy:  0.8240740740740741\n",
            "Batch loss:  0.4509503\n",
            "Batch accuracy:  0.898936170212766\n",
            "Batch loss:  0.31180942\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.27148348\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.29148054\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.31989393\n",
            "Batch accuracy:  0.7227272727272728\n",
            "Batch loss:  0.630742\n",
            "Batch accuracy:  0.8949579831932774\n",
            "Batch loss:  0.23306605\n",
            "Batch accuracy:  0.7596899224806202\n",
            "Batch loss:  0.5230088\n",
            "Epoch ended\n",
            "Total loss:  38.95473067462444\n",
            "Average batch loss:  0.35413391522385856\n",
            "Starting epoch:  52\n",
            "Batch accuracy:  0.8292682926829268\n",
            "Batch loss:  0.3163936\n",
            "Batch accuracy:  0.8653846153846154\n",
            "Batch loss:  0.35397223\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.23893797\n",
            "Batch accuracy:  0.8614864864864865\n",
            "Batch loss:  0.32145587\n",
            "Batch accuracy:  0.8842592592592593\n",
            "Batch loss:  0.30780122\n",
            "Batch accuracy:  0.8670212765957447\n",
            "Batch loss:  0.2944338\n",
            "Batch accuracy:  0.8616071428571429\n",
            "Batch loss:  0.35791808\n",
            "Batch accuracy:  0.8689320388349514\n",
            "Batch loss:  0.3516572\n",
            "Batch accuracy:  0.8214285714285714\n",
            "Batch loss:  0.37471834\n",
            "Batch accuracy:  0.8545454545454545\n",
            "Batch loss:  0.37442902\n",
            "Batch accuracy:  0.8235294117647058\n",
            "Batch loss:  0.4627728\n",
            "Batch accuracy:  0.8178294573643411\n",
            "Batch loss:  0.3710331\n",
            "Epoch ended\n",
            "Total loss:  38.81205327808857\n",
            "Average batch loss:  0.3528368479826234\n",
            "Starting epoch:  53\n",
            "Batch accuracy:  0.9065040650406504\n",
            "Batch loss:  0.31545505\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.326176\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.36298823\n",
            "Batch accuracy:  0.8074324324324325\n",
            "Batch loss:  0.3527734\n",
            "Batch accuracy:  0.8518518518518519\n",
            "Batch loss:  0.31443635\n",
            "Batch accuracy:  0.8563829787234043\n",
            "Batch loss:  0.37498242\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.3396365\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.30309555\n",
            "Batch accuracy:  0.9030612244897959\n",
            "Batch loss:  0.257799\n",
            "Batch accuracy:  0.85\n",
            "Batch loss:  0.3887897\n",
            "Batch accuracy:  0.8277310924369747\n",
            "Batch loss:  0.36600116\n",
            "Batch accuracy:  0.8643410852713178\n",
            "Batch loss:  0.33252174\n",
            "Epoch ended\n",
            "Total loss:  37.437499925494194\n",
            "Average batch loss:  0.3403409084135836\n",
            "Starting epoch:  54\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.31629038\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3254212\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.24327427\n",
            "Batch accuracy:  0.7668918918918919\n",
            "Batch loss:  0.4750971\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.32184657\n",
            "Batch accuracy:  0.8563829787234043\n",
            "Batch loss:  0.33920902\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.37502423\n",
            "Batch accuracy:  0.8398058252427184\n",
            "Batch loss:  0.32334614\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.33150497\n",
            "Batch accuracy:  0.8636363636363636\n",
            "Batch loss:  0.33715445\n",
            "Batch accuracy:  0.8739495798319328\n",
            "Batch loss:  0.3322754\n",
            "Batch accuracy:  0.8565891472868217\n",
            "Batch loss:  0.4248447\n",
            "Epoch ended\n",
            "Total loss:  37.44701471924782\n",
            "Average batch loss:  0.34042740653861653\n",
            "Starting epoch:  55\n",
            "Batch accuracy:  0.8495934959349594\n",
            "Batch loss:  0.31783503\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.37613085\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.20120148\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.31144008\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.38083723\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.3539559\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.39088154\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.22573785\n",
            "Batch accuracy:  0.8826530612244898\n",
            "Batch loss:  0.3348827\n",
            "Batch accuracy:  0.7681818181818182\n",
            "Batch loss:  0.45292214\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.38198686\n",
            "Batch accuracy:  0.7829457364341085\n",
            "Batch loss:  0.402929\n",
            "Epoch ended\n",
            "Total loss:  37.26754155755043\n",
            "Average batch loss:  0.33879583234136756\n",
            "Starting epoch:  56\n",
            "Batch accuracy:  0.8495934959349594\n",
            "Batch loss:  0.36061007\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.4123319\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.31072295\n",
            "Batch accuracy:  0.8412162162162162\n",
            "Batch loss:  0.39959505\n",
            "Batch accuracy:  0.9166666666666666\n",
            "Batch loss:  0.28522718\n",
            "Batch accuracy:  0.8404255319148937\n",
            "Batch loss:  0.3551291\n",
            "Batch accuracy:  0.7991071428571429\n",
            "Batch loss:  0.48471704\n",
            "Batch accuracy:  0.8689320388349514\n",
            "Batch loss:  0.28419477\n",
            "Batch accuracy:  0.8673469387755102\n",
            "Batch loss:  0.29026192\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.41414577\n",
            "Batch accuracy:  0.8865546218487395\n",
            "Batch loss:  0.28849238\n",
            "Batch accuracy:  0.7635658914728682\n",
            "Batch loss:  0.46866512\n",
            "Epoch ended\n",
            "Total loss:  38.0287364423275\n",
            "Average batch loss:  0.34571578583934093\n",
            "Starting epoch:  57\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.40183413\n",
            "Batch accuracy:  0.8141025641025641\n",
            "Batch loss:  0.3389979\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.30753317\n",
            "Batch accuracy:  0.8074324324324325\n",
            "Batch loss:  0.48919895\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.36147314\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.3548372\n",
            "Batch accuracy:  0.8660714285714286\n",
            "Batch loss:  0.2693605\n",
            "Batch accuracy:  0.7815533980582524\n",
            "Batch loss:  0.46325725\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.31136027\n",
            "Batch accuracy:  0.7363636363636363\n",
            "Batch loss:  0.5500197\n",
            "Batch accuracy:  0.8277310924369747\n",
            "Batch loss:  0.38729867\n",
            "Batch accuracy:  0.8178294573643411\n",
            "Batch loss:  0.31442562\n",
            "Epoch ended\n",
            "Total loss:  37.65096877515316\n",
            "Average batch loss:  0.3422815343195742\n",
            "Starting epoch:  58\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.29523018\n",
            "Batch accuracy:  0.8782051282051282\n",
            "Batch loss:  0.3086389\n",
            "Batch accuracy:  0.9121621621621622\n",
            "Batch loss:  0.24999477\n",
            "Batch accuracy:  0.8817567567567568\n",
            "Batch loss:  0.374826\n",
            "Batch accuracy:  0.8657407407407407\n",
            "Batch loss:  0.34378055\n",
            "Batch accuracy:  0.8404255319148937\n",
            "Batch loss:  0.32754853\n",
            "Batch accuracy:  0.8526785714285714\n",
            "Batch loss:  0.32598916\n",
            "Batch accuracy:  0.8398058252427184\n",
            "Batch loss:  0.31179336\n",
            "Batch accuracy:  0.8469387755102041\n",
            "Batch loss:  0.33018473\n",
            "Batch accuracy:  0.7772727272727272\n",
            "Batch loss:  0.567935\n",
            "Batch accuracy:  0.7983193277310925\n",
            "Batch loss:  0.39714435\n",
            "Batch accuracy:  0.8527131782945736\n",
            "Batch loss:  0.37199625\n",
            "Epoch ended\n",
            "Total loss:  37.341777354478836\n",
            "Average batch loss:  0.33947070322253486\n",
            "Starting epoch:  59\n",
            "Batch accuracy:  0.8902439024390244\n",
            "Batch loss:  0.35859546\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.31449965\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.30173045\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.3176149\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.44685194\n",
            "Batch accuracy:  0.8457446808510638\n",
            "Batch loss:  0.3341905\n",
            "Batch accuracy:  0.8303571428571429\n",
            "Batch loss:  0.42089146\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.32429734\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.30261794\n",
            "Batch accuracy:  0.7818181818181819\n",
            "Batch loss:  0.36697942\n",
            "Batch accuracy:  0.8529411764705882\n",
            "Batch loss:  0.35863942\n",
            "Batch accuracy:  0.8488372093023255\n",
            "Batch loss:  0.32783216\n",
            "Epoch ended\n",
            "Total loss:  37.943208783864975\n",
            "Average batch loss:  0.34493826167149977\n",
            "Starting epoch:  60\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.3092503\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.3416857\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.24657786\n",
            "Batch accuracy:  0.75\n",
            "Batch loss:  0.48083848\n",
            "Batch accuracy:  0.8888888888888888\n",
            "Batch loss:  0.3505359\n",
            "Batch accuracy:  0.9148936170212766\n",
            "Batch loss:  0.27950564\n",
            "Batch accuracy:  0.8928571428571429\n",
            "Batch loss:  0.3079834\n",
            "Batch accuracy:  0.912621359223301\n",
            "Batch loss:  0.27818158\n",
            "Batch accuracy:  0.8673469387755102\n",
            "Batch loss:  0.3108293\n",
            "Batch accuracy:  0.8727272727272727\n",
            "Batch loss:  0.3019875\n",
            "Batch accuracy:  0.8319327731092437\n",
            "Batch loss:  0.31428784\n",
            "Batch accuracy:  0.8294573643410853\n",
            "Batch loss:  0.36280856\n",
            "Epoch ended\n",
            "Total loss:  36.87696582078934\n",
            "Average batch loss:  0.3352451438253576\n",
            "Starting epoch:  61\n",
            "Batch accuracy:  0.8943089430894309\n",
            "Batch loss:  0.27943206\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.37824255\n",
            "Batch accuracy:  0.9324324324324325\n",
            "Batch loss:  0.2053648\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.36983043\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.31118608\n",
            "Batch accuracy:  0.8829787234042553\n",
            "Batch loss:  0.36925048\n",
            "Batch accuracy:  0.8482142857142857\n",
            "Batch loss:  0.32886973\n",
            "Batch accuracy:  0.8543689320388349\n",
            "Batch loss:  0.34055638\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.34858522\n",
            "Batch accuracy:  0.8409090909090909\n",
            "Batch loss:  0.33921054\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.3412987\n",
            "Batch accuracy:  0.8798449612403101\n",
            "Batch loss:  0.30691636\n",
            "Epoch ended\n",
            "Total loss:  36.595942601561546\n",
            "Average batch loss:  0.33269038728692313\n",
            "Starting epoch:  62\n",
            "Batch accuracy:  0.8861788617886179\n",
            "Batch loss:  0.2758642\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.34601274\n",
            "Batch accuracy:  0.9324324324324325\n",
            "Batch loss:  0.16834264\n",
            "Batch accuracy:  0.8006756756756757\n",
            "Batch loss:  0.39941567\n",
            "Batch accuracy:  0.9027777777777778\n",
            "Batch loss:  0.27294102\n",
            "Batch accuracy:  0.8191489361702128\n",
            "Batch loss:  0.47074103\n",
            "Batch accuracy:  0.8482142857142857\n",
            "Batch loss:  0.33505246\n",
            "Batch accuracy:  0.9223300970873787\n",
            "Batch loss:  0.28160608\n",
            "Batch accuracy:  0.8826530612244898\n",
            "Batch loss:  0.30966723\n",
            "Batch accuracy:  0.8681818181818182\n",
            "Batch loss:  0.3349665\n",
            "Batch accuracy:  0.865546218487395\n",
            "Batch loss:  0.31362456\n",
            "Batch accuracy:  0.8565891472868217\n",
            "Batch loss:  0.38326082\n",
            "Epoch ended\n",
            "Total loss:  36.77653932571411\n",
            "Average batch loss:  0.3343321756883101\n",
            "Starting epoch:  63\n",
            "Batch accuracy:  0.8943089430894309\n",
            "Batch loss:  0.27380744\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.3425797\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.2868327\n",
            "Batch accuracy:  0.8614864864864865\n",
            "Batch loss:  0.39274782\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.38378695\n",
            "Batch accuracy:  0.8776595744680851\n",
            "Batch loss:  0.24953344\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.61119044\n",
            "Batch accuracy:  0.9029126213592233\n",
            "Batch loss:  0.304505\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.3699774\n",
            "Batch accuracy:  0.7909090909090909\n",
            "Batch loss:  0.4131072\n",
            "Batch accuracy:  0.8823529411764706\n",
            "Batch loss:  0.35258755\n",
            "Batch accuracy:  0.875968992248062\n",
            "Batch loss:  0.36381996\n",
            "Epoch ended\n",
            "Total loss:  37.270729184150696\n",
            "Average batch loss:  0.3388248107650063\n",
            "Starting epoch:  64\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.3250225\n",
            "Batch accuracy:  0.9102564102564102\n",
            "Batch loss:  0.24103618\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.3591113\n",
            "Batch accuracy:  0.8006756756756757\n",
            "Batch loss:  0.53385484\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.32126632\n",
            "Batch accuracy:  0.8085106382978723\n",
            "Batch loss:  0.32273307\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.34513253\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.28163078\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.4137416\n",
            "Batch accuracy:  0.8727272727272727\n",
            "Batch loss:  0.35580552\n",
            "Batch accuracy:  0.8781512605042017\n",
            "Batch loss:  0.33120468\n",
            "Batch accuracy:  0.875968992248062\n",
            "Batch loss:  0.33754912\n",
            "Epoch ended\n",
            "Total loss:  36.792924985289574\n",
            "Average batch loss:  0.3344811362299052\n",
            "Starting epoch:  65\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.35236764\n",
            "Batch accuracy:  0.9102564102564102\n",
            "Batch loss:  0.25887188\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.28276864\n",
            "Batch accuracy:  0.8412162162162162\n",
            "Batch loss:  0.43370205\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.38777614\n",
            "Batch accuracy:  0.9095744680851063\n",
            "Batch loss:  0.23980196\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.37771168\n",
            "Batch accuracy:  0.8737864077669902\n",
            "Batch loss:  0.2905135\n",
            "Batch accuracy:  0.8418367346938775\n",
            "Batch loss:  0.39426053\n",
            "Batch accuracy:  0.8863636363636364\n",
            "Batch loss:  0.29227808\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.36135015\n",
            "Batch accuracy:  0.872093023255814\n",
            "Batch loss:  0.3393866\n",
            "Epoch ended\n",
            "Total loss:  36.5092443972826\n",
            "Average batch loss:  0.33190222179347817\n",
            "Starting epoch:  66\n",
            "Batch accuracy:  0.8170731707317073\n",
            "Batch loss:  0.49922025\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.33171853\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.2248366\n",
            "Batch accuracy:  0.8445945945945946\n",
            "Batch loss:  0.2975577\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.30513927\n",
            "Batch accuracy:  0.8829787234042553\n",
            "Batch loss:  0.29329222\n",
            "Batch accuracy:  0.8169642857142857\n",
            "Batch loss:  0.2858619\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.32102987\n",
            "Batch accuracy:  0.8928571428571429\n",
            "Batch loss:  0.27295533\n",
            "Batch accuracy:  0.85\n",
            "Batch loss:  0.2896525\n",
            "Batch accuracy:  0.9033613445378151\n",
            "Batch loss:  0.34405565\n",
            "Batch accuracy:  0.8837209302325582\n",
            "Batch loss:  0.32608494\n",
            "Epoch ended\n",
            "Total loss:  36.31710796058178\n",
            "Average batch loss:  0.3301555269143798\n",
            "Starting epoch:  67\n",
            "Batch accuracy:  0.8780487804878049\n",
            "Batch loss:  0.31418806\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.37240264\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.34275445\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.4201913\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.34786522\n",
            "Batch accuracy:  0.9042553191489362\n",
            "Batch loss:  0.256145\n",
            "Batch accuracy:  0.84375\n",
            "Batch loss:  0.4333504\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.3190201\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.3295344\n",
            "Batch accuracy:  0.7954545454545454\n",
            "Batch loss:  0.5205921\n",
            "Batch accuracy:  0.8823529411764706\n",
            "Batch loss:  0.3121051\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.40334943\n",
            "Epoch ended\n",
            "Total loss:  35.8792579472065\n",
            "Average batch loss:  0.3261750722473318\n",
            "Starting epoch:  68\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.361066\n",
            "Batch accuracy:  0.9102564102564102\n",
            "Batch loss:  0.28232697\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.23750606\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.38002712\n",
            "Batch accuracy:  0.8796296296296297\n",
            "Batch loss:  0.332924\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.3489319\n",
            "Batch accuracy:  0.8482142857142857\n",
            "Batch loss:  0.47537008\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.2650861\n",
            "Batch accuracy:  0.8979591836734694\n",
            "Batch loss:  0.2555972\n",
            "Batch accuracy:  0.8772727272727273\n",
            "Batch loss:  0.3188333\n",
            "Batch accuracy:  0.819327731092437\n",
            "Batch loss:  0.41545957\n",
            "Batch accuracy:  0.8682170542635659\n",
            "Batch loss:  0.30891955\n",
            "Epoch ended\n",
            "Total loss:  36.97356717288494\n",
            "Average batch loss:  0.3361233379353176\n",
            "Starting epoch:  69\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.35577422\n",
            "Batch accuracy:  0.8589743589743589\n",
            "Batch loss:  0.37544408\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.22721227\n",
            "Batch accuracy:  0.8108108108108109\n",
            "Batch loss:  0.60606676\n",
            "Batch accuracy:  0.8657407407407407\n",
            "Batch loss:  0.37485886\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.278816\n",
            "Batch accuracy:  0.8616071428571429\n",
            "Batch loss:  0.36186102\n",
            "Batch accuracy:  0.9029126213592233\n",
            "Batch loss:  0.25152168\n",
            "Batch accuracy:  0.9081632653061225\n",
            "Batch loss:  0.28747642\n",
            "Batch accuracy:  0.8681818181818182\n",
            "Batch loss:  0.31340486\n",
            "Batch accuracy:  0.8907563025210085\n",
            "Batch loss:  0.20157345\n",
            "Batch accuracy:  0.8449612403100775\n",
            "Batch loss:  0.33825797\n",
            "Epoch ended\n",
            "Total loss:  36.011259868741035\n",
            "Average batch loss:  0.32737508971582757\n",
            "Starting epoch:  70\n",
            "Batch accuracy:  0.8658536585365854\n",
            "Batch loss:  0.27969795\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.36499888\n",
            "Batch accuracy:  0.9391891891891891\n",
            "Batch loss:  0.25981995\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.31222525\n",
            "Batch accuracy:  0.8425925925925926\n",
            "Batch loss:  0.31723154\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.36756295\n",
            "Batch accuracy:  0.8258928571428571\n",
            "Batch loss:  0.48199493\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.25498325\n",
            "Batch accuracy:  0.8469387755102041\n",
            "Batch loss:  0.3267745\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.32670912\n",
            "Batch accuracy:  0.8739495798319328\n",
            "Batch loss:  0.36848572\n",
            "Batch accuracy:  0.8798449612403101\n",
            "Batch loss:  0.25572595\n",
            "Epoch ended\n",
            "Total loss:  35.19348105788231\n",
            "Average batch loss:  0.31994073688983915\n",
            "Starting epoch:  71\n",
            "Batch accuracy:  0.8739837398373984\n",
            "Batch loss:  0.36203215\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.28304902\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.23506038\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.3624083\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.3681373\n",
            "Batch accuracy:  0.8936170212765957\n",
            "Batch loss:  0.36268264\n",
            "Batch accuracy:  0.8258928571428571\n",
            "Batch loss:  0.37493536\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.24235511\n",
            "Batch accuracy:  0.8928571428571429\n",
            "Batch loss:  0.29653516\n",
            "Batch accuracy:  0.85\n",
            "Batch loss:  0.35737392\n",
            "Batch accuracy:  0.8991596638655462\n",
            "Batch loss:  0.29149482\n",
            "Batch accuracy:  0.8604651162790697\n",
            "Batch loss:  0.33991843\n",
            "Epoch ended\n",
            "Total loss:  36.56343820691109\n",
            "Average batch loss:  0.3323948927901008\n",
            "Starting epoch:  72\n",
            "Batch accuracy:  0.8617886178861789\n",
            "Batch loss:  0.35245457\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.33044124\n",
            "Batch accuracy:  0.9391891891891891\n",
            "Batch loss:  0.20996013\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.44300094\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.31747097\n",
            "Batch accuracy:  0.824468085106383\n",
            "Batch loss:  0.41712713\n",
            "Batch accuracy:  0.9107142857142857\n",
            "Batch loss:  0.24531303\n",
            "Batch accuracy:  0.9029126213592233\n",
            "Batch loss:  0.23291197\n",
            "Batch accuracy:  0.7755102040816326\n",
            "Batch loss:  0.46009943\n",
            "Batch accuracy:  0.8954545454545455\n",
            "Batch loss:  0.3643588\n",
            "Batch accuracy:  0.8613445378151261\n",
            "Batch loss:  0.35095987\n",
            "Batch accuracy:  0.8488372093023255\n",
            "Batch loss:  0.48053005\n",
            "Epoch ended\n",
            "Total loss:  35.50661934912205\n",
            "Average batch loss:  0.32278744862838227\n",
            "Starting epoch:  73\n",
            "Batch accuracy:  0.8658536585365854\n",
            "Batch loss:  0.3422926\n",
            "Batch accuracy:  0.8205128205128205\n",
            "Batch loss:  0.41267052\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.22003081\n",
            "Batch accuracy:  0.8344594594594594\n",
            "Batch loss:  0.38987896\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.29321617\n",
            "Batch accuracy:  0.9148936170212766\n",
            "Batch loss:  0.1916271\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.3224915\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.42194086\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.33771613\n",
            "Batch accuracy:  0.85\n",
            "Batch loss:  0.44118583\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.30604708\n",
            "Batch accuracy:  0.810077519379845\n",
            "Batch loss:  0.35110524\n",
            "Epoch ended\n",
            "Total loss:  35.32267242670059\n",
            "Average batch loss:  0.3211152038790963\n",
            "Starting epoch:  74\n",
            "Batch accuracy:  0.8455284552845529\n",
            "Batch loss:  0.3653222\n",
            "Batch accuracy:  0.9038461538461539\n",
            "Batch loss:  0.30692622\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.31228933\n",
            "Batch accuracy:  0.7601351351351351\n",
            "Batch loss:  0.5511221\n",
            "Batch accuracy:  0.8842592592592593\n",
            "Batch loss:  0.2987297\n",
            "Batch accuracy:  0.925531914893617\n",
            "Batch loss:  0.20398296\n",
            "Batch accuracy:  0.7901785714285714\n",
            "Batch loss:  0.36866638\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.27413726\n",
            "Batch accuracy:  0.9081632653061225\n",
            "Batch loss:  0.24592654\n",
            "Batch accuracy:  0.8318181818181818\n",
            "Batch loss:  0.40357512\n",
            "Batch accuracy:  0.8445378151260504\n",
            "Batch loss:  0.41785732\n",
            "Batch accuracy:  0.8837209302325582\n",
            "Batch loss:  0.34386224\n",
            "Epoch ended\n",
            "Total loss:  34.858364790678024\n",
            "Average batch loss:  0.3168942253698002\n",
            "Starting epoch:  75\n",
            "Batch accuracy:  0.8414634146341463\n",
            "Batch loss:  0.34595767\n",
            "Batch accuracy:  0.8910256410256411\n",
            "Batch loss:  0.40891588\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.26253614\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.35523364\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.36439547\n",
            "Batch accuracy:  0.851063829787234\n",
            "Batch loss:  0.30145317\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.30818218\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.3160732\n",
            "Batch accuracy:  0.8622448979591837\n",
            "Batch loss:  0.3521104\n",
            "Batch accuracy:  0.8318181818181818\n",
            "Batch loss:  0.39942864\n",
            "Batch accuracy:  0.8781512605042017\n",
            "Batch loss:  0.34018493\n",
            "Batch accuracy:  0.8643410852713178\n",
            "Batch loss:  0.33416143\n",
            "Epoch ended\n",
            "Total loss:  35.128549098968506\n",
            "Average batch loss:  0.31935044635425913\n",
            "Starting epoch:  76\n",
            "Batch accuracy:  0.8617886178861789\n",
            "Batch loss:  0.34118035\n",
            "Batch accuracy:  0.8653846153846154\n",
            "Batch loss:  0.3968448\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.27776557\n",
            "Batch accuracy:  0.8209459459459459\n",
            "Batch loss:  0.40750092\n",
            "Batch accuracy:  0.8935185185185185\n",
            "Batch loss:  0.27570394\n",
            "Batch accuracy:  0.9042553191489362\n",
            "Batch loss:  0.30423835\n",
            "Batch accuracy:  0.8839285714285714\n",
            "Batch loss:  0.32355168\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.26525584\n",
            "Batch accuracy:  0.8826530612244898\n",
            "Batch loss:  0.26994386\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.28209034\n",
            "Batch accuracy:  0.8487394957983193\n",
            "Batch loss:  0.31116864\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.3718471\n",
            "Epoch ended\n",
            "Total loss:  35.39083859324455\n",
            "Average batch loss:  0.3217348963022232\n",
            "Starting epoch:  77\n",
            "Batch accuracy:  0.8821138211382114\n",
            "Batch loss:  0.3443201\n",
            "Batch accuracy:  0.8910256410256411\n",
            "Batch loss:  0.26643625\n",
            "Batch accuracy:  0.8581081081081081\n",
            "Batch loss:  0.34150895\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.34759492\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.35206342\n",
            "Batch accuracy:  0.9042553191489362\n",
            "Batch loss:  0.25755316\n",
            "Batch accuracy:  0.8258928571428571\n",
            "Batch loss:  0.49298328\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.34642473\n",
            "Batch accuracy:  0.8520408163265306\n",
            "Batch loss:  0.31882423\n",
            "Batch accuracy:  0.8409090909090909\n",
            "Batch loss:  0.3074913\n",
            "Batch accuracy:  0.9159663865546218\n",
            "Batch loss:  0.2529045\n",
            "Batch accuracy:  0.7829457364341085\n",
            "Batch loss:  0.44994578\n",
            "Epoch ended\n",
            "Total loss:  35.365209236741066\n",
            "Average batch loss:  0.3215019021521915\n",
            "Starting epoch:  78\n",
            "Batch accuracy:  0.8943089430894309\n",
            "Batch loss:  0.2395832\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.30368474\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.31745416\n",
            "Batch accuracy:  0.8614864864864865\n",
            "Batch loss:  0.37943047\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.38187993\n",
            "Batch accuracy:  0.8882978723404256\n",
            "Batch loss:  0.31987244\n",
            "Batch accuracy:  0.84375\n",
            "Batch loss:  0.41127938\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.30247694\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.33894867\n",
            "Batch accuracy:  0.8954545454545455\n",
            "Batch loss:  0.26802114\n",
            "Batch accuracy:  0.7983193277310925\n",
            "Batch loss:  0.38649228\n",
            "Batch accuracy:  0.813953488372093\n",
            "Batch loss:  0.35883072\n",
            "Epoch ended\n",
            "Total loss:  35.139914751052856\n",
            "Average batch loss:  0.3194537704641169\n",
            "Starting epoch:  79\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.31647873\n",
            "Batch accuracy:  0.9294871794871795\n",
            "Batch loss:  0.15819013\n",
            "Batch accuracy:  0.9459459459459459\n",
            "Batch loss:  0.20622396\n",
            "Batch accuracy:  0.8817567567567568\n",
            "Batch loss:  0.2583976\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.36534703\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.29026297\n",
            "Batch accuracy:  0.8616071428571429\n",
            "Batch loss:  0.37523013\n",
            "Batch accuracy:  0.8786407766990292\n",
            "Batch loss:  0.37016347\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.37694475\n",
            "Batch accuracy:  0.8727272727272727\n",
            "Batch loss:  0.38089117\n",
            "Batch accuracy:  0.9117647058823529\n",
            "Batch loss:  0.24249408\n",
            "Batch accuracy:  0.8953488372093024\n",
            "Batch loss:  0.28389075\n",
            "Epoch ended\n",
            "Total loss:  35.23773629963398\n",
            "Average batch loss:  0.3203430572693998\n",
            "Starting epoch:  80\n",
            "Batch accuracy:  0.8699186991869918\n",
            "Batch loss:  0.35240376\n",
            "Batch accuracy:  0.8461538461538461\n",
            "Batch loss:  0.35616326\n",
            "Batch accuracy:  0.9121621621621622\n",
            "Batch loss:  0.31070676\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.3296052\n",
            "Batch accuracy:  0.9027777777777778\n",
            "Batch loss:  0.27610138\n",
            "Batch accuracy:  0.9361702127659575\n",
            "Batch loss:  0.29574975\n",
            "Batch accuracy:  0.8169642857142857\n",
            "Batch loss:  0.37695056\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.23377159\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.33746436\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.34313944\n",
            "Batch accuracy:  0.8781512605042017\n",
            "Batch loss:  0.31883225\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.3861542\n",
            "Epoch ended\n",
            "Total loss:  35.63953447341919\n",
            "Average batch loss:  0.32399576794017443\n",
            "Starting epoch:  81\n",
            "Batch accuracy:  0.9186991869918699\n",
            "Batch loss:  0.27737334\n",
            "Batch accuracy:  0.8653846153846154\n",
            "Batch loss:  0.27585113\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.33245277\n",
            "Batch accuracy:  0.8513513513513513\n",
            "Batch loss:  0.3091606\n",
            "Batch accuracy:  0.8564814814814815\n",
            "Batch loss:  0.35481125\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.24612392\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.2658837\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.34072298\n",
            "Batch accuracy:  0.8316326530612245\n",
            "Batch loss:  0.38082325\n",
            "Batch accuracy:  0.8045454545454546\n",
            "Batch loss:  0.4101251\n",
            "Batch accuracy:  0.8739495798319328\n",
            "Batch loss:  0.32081792\n",
            "Batch accuracy:  0.8178294573643411\n",
            "Batch loss:  0.40844202\n",
            "Epoch ended\n",
            "Total loss:  34.7145883589983\n",
            "Average batch loss:  0.3155871668999845\n",
            "Starting epoch:  82\n",
            "Batch accuracy:  0.8048780487804879\n",
            "Batch loss:  0.42731822\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.21988586\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.2645463\n",
            "Batch accuracy:  0.8614864864864865\n",
            "Batch loss:  0.2814629\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.2544656\n",
            "Batch accuracy:  0.8404255319148937\n",
            "Batch loss:  0.3484038\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.30493808\n",
            "Batch accuracy:  0.8592233009708737\n",
            "Batch loss:  0.29918316\n",
            "Batch accuracy:  0.8877551020408163\n",
            "Batch loss:  0.30544788\n",
            "Batch accuracy:  0.8863636363636364\n",
            "Batch loss:  0.24512407\n",
            "Batch accuracy:  0.907563025210084\n",
            "Batch loss:  0.26265094\n",
            "Batch accuracy:  0.686046511627907\n",
            "Batch loss:  0.672649\n",
            "Epoch ended\n",
            "Total loss:  35.120537891983986\n",
            "Average batch loss:  0.3192776171998544\n",
            "Starting epoch:  83\n",
            "Batch accuracy:  0.8902439024390244\n",
            "Batch loss:  0.29434204\n",
            "Batch accuracy:  0.8782051282051282\n",
            "Batch loss:  0.29710156\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.2098527\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.40846038\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.3905338\n",
            "Batch accuracy:  0.925531914893617\n",
            "Batch loss:  0.18471564\n",
            "Batch accuracy:  0.9017857142857143\n",
            "Batch loss:  0.26017037\n",
            "Batch accuracy:  0.8689320388349514\n",
            "Batch loss:  0.33770978\n",
            "Batch accuracy:  0.8214285714285714\n",
            "Batch loss:  0.3371424\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.36201888\n",
            "Batch accuracy:  0.907563025210084\n",
            "Batch loss:  0.2232161\n",
            "Batch accuracy:  0.8565891472868217\n",
            "Batch loss:  0.29357097\n",
            "Epoch ended\n",
            "Total loss:  33.7472822368145\n",
            "Average batch loss:  0.3067934748801318\n",
            "Starting epoch:  84\n",
            "Batch accuracy:  0.8699186991869918\n",
            "Batch loss:  0.37802345\n",
            "Batch accuracy:  0.9294871794871795\n",
            "Batch loss:  0.22143173\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.27141902\n",
            "Batch accuracy:  0.8378378378378378\n",
            "Batch loss:  0.4065994\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.28862303\n",
            "Batch accuracy:  0.8936170212765957\n",
            "Batch loss:  0.23923156\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.38829097\n",
            "Batch accuracy:  0.8155339805825242\n",
            "Batch loss:  0.33837584\n",
            "Batch accuracy:  0.9030612244897959\n",
            "Batch loss:  0.21782076\n",
            "Batch accuracy:  0.8590909090909091\n",
            "Batch loss:  0.31008357\n",
            "Batch accuracy:  0.8865546218487395\n",
            "Batch loss:  0.2526798\n",
            "Batch accuracy:  0.8992248062015504\n",
            "Batch loss:  0.3622249\n",
            "Epoch ended\n",
            "Total loss:  34.690224662423134\n",
            "Average batch loss:  0.3153656787493012\n",
            "Starting epoch:  85\n",
            "Batch accuracy:  0.9105691056910569\n",
            "Batch loss:  0.28896862\n",
            "Batch accuracy:  0.8717948717948718\n",
            "Batch loss:  0.3680797\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.22495489\n",
            "Batch accuracy:  0.8716216216216216\n",
            "Batch loss:  0.3496158\n",
            "Batch accuracy:  0.8703703703703703\n",
            "Batch loss:  0.31801942\n",
            "Batch accuracy:  0.898936170212766\n",
            "Batch loss:  0.2651837\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.31915674\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.30564198\n",
            "Batch accuracy:  0.8826530612244898\n",
            "Batch loss:  0.29386342\n",
            "Batch accuracy:  0.8363636363636363\n",
            "Batch loss:  0.40871423\n",
            "Batch accuracy:  0.8949579831932774\n",
            "Batch loss:  0.25202373\n",
            "Batch accuracy:  0.8565891472868217\n",
            "Batch loss:  0.34921348\n",
            "Epoch ended\n",
            "Total loss:  34.890446692705154\n",
            "Average batch loss:  0.31718587902459233\n",
            "Starting epoch:  86\n",
            "Batch accuracy:  0.8495934959349594\n",
            "Batch loss:  0.30518526\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.3540679\n",
            "Batch accuracy:  0.9459459459459459\n",
            "Batch loss:  0.20735759\n",
            "Batch accuracy:  0.8783783783783784\n",
            "Batch loss:  0.30610308\n",
            "Batch accuracy:  0.9074074074074074\n",
            "Batch loss:  0.33486736\n",
            "Batch accuracy:  0.8351063829787234\n",
            "Batch loss:  0.31546432\n",
            "Batch accuracy:  0.8883928571428571\n",
            "Batch loss:  0.2662539\n",
            "Batch accuracy:  0.912621359223301\n",
            "Batch loss:  0.2105293\n",
            "Batch accuracy:  0.8877551020408163\n",
            "Batch loss:  0.24354061\n",
            "Batch accuracy:  0.8636363636363636\n",
            "Batch loss:  0.38125464\n",
            "Batch accuracy:  0.8361344537815126\n",
            "Batch loss:  0.40396532\n",
            "Batch accuracy:  0.8953488372093024\n",
            "Batch loss:  0.26363686\n",
            "Epoch ended\n",
            "Total loss:  34.082617193460464\n",
            "Average batch loss:  0.3098419744860042\n",
            "Starting epoch:  87\n",
            "Batch accuracy:  0.8536585365853658\n",
            "Batch loss:  0.28540558\n",
            "Batch accuracy:  0.8397435897435898\n",
            "Batch loss:  0.33622274\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.3012456\n",
            "Batch accuracy:  0.8547297297297297\n",
            "Batch loss:  0.40643403\n",
            "Batch accuracy:  0.8518518518518519\n",
            "Batch loss:  0.43572405\n",
            "Batch accuracy:  0.9042553191489362\n",
            "Batch loss:  0.2718258\n",
            "Batch accuracy:  0.8883928571428571\n",
            "Batch loss:  0.2884542\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.3483534\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.34220508\n",
            "Batch accuracy:  0.8136363636363636\n",
            "Batch loss:  0.34642348\n",
            "Batch accuracy:  0.8487394957983193\n",
            "Batch loss:  0.33635676\n",
            "Batch accuracy:  0.8798449612403101\n",
            "Batch loss:  0.30541128\n",
            "Epoch ended\n",
            "Total loss:  34.61655031144619\n",
            "Average batch loss:  0.3146959119222381\n",
            "Starting epoch:  88\n",
            "Batch accuracy:  0.9065040650406504\n",
            "Batch loss:  0.23949836\n",
            "Batch accuracy:  0.9038461538461539\n",
            "Batch loss:  0.22216105\n",
            "Batch accuracy:  0.9324324324324325\n",
            "Batch loss:  0.2111438\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.21586844\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.365545\n",
            "Batch accuracy:  0.898936170212766\n",
            "Batch loss:  0.25293028\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.3304483\n",
            "Batch accuracy:  0.9077669902912622\n",
            "Batch loss:  0.22730775\n",
            "Batch accuracy:  0.923469387755102\n",
            "Batch loss:  0.19753373\n",
            "Batch accuracy:  0.8818181818181818\n",
            "Batch loss:  0.23913139\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.3681218\n",
            "Batch accuracy:  0.8178294573643411\n",
            "Batch loss:  0.53253907\n",
            "Epoch ended\n",
            "Total loss:  35.0728325098753\n",
            "Average batch loss:  0.31884393190795723\n",
            "Starting epoch:  89\n",
            "Batch accuracy:  0.8983739837398373\n",
            "Batch loss:  0.29986697\n",
            "Batch accuracy:  0.9038461538461539\n",
            "Batch loss:  0.24531277\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.24604014\n",
            "Batch accuracy:  0.8040540540540541\n",
            "Batch loss:  0.65511245\n",
            "Batch accuracy:  0.8611111111111112\n",
            "Batch loss:  0.3275761\n",
            "Batch accuracy:  0.8617021276595744\n",
            "Batch loss:  0.3686366\n",
            "Batch accuracy:  0.8973214285714286\n",
            "Batch loss:  0.2824301\n",
            "Batch accuracy:  0.8495145631067961\n",
            "Batch loss:  0.33773685\n",
            "Batch accuracy:  0.8979591836734694\n",
            "Batch loss:  0.3002682\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.32095873\n",
            "Batch accuracy:  0.8235294117647058\n",
            "Batch loss:  0.40331128\n",
            "Batch accuracy:  0.8410852713178295\n",
            "Batch loss:  0.3887588\n",
            "Epoch ended\n",
            "Total loss:  34.115294978022575\n",
            "Average batch loss:  0.31013904525475067\n",
            "Starting epoch:  90\n",
            "Batch accuracy:  0.9105691056910569\n",
            "Batch loss:  0.23223294\n",
            "Batch accuracy:  0.8846153846153846\n",
            "Batch loss:  0.29165846\n",
            "Batch accuracy:  0.777027027027027\n",
            "Batch loss:  0.4914851\n",
            "Batch accuracy:  0.8581081081081081\n",
            "Batch loss:  0.32792073\n",
            "Batch accuracy:  0.9074074074074074\n",
            "Batch loss:  0.2767505\n",
            "Batch accuracy:  0.8882978723404256\n",
            "Batch loss:  0.24431157\n",
            "Batch accuracy:  0.8482142857142857\n",
            "Batch loss:  0.29824346\n",
            "Batch accuracy:  0.8980582524271845\n",
            "Batch loss:  0.27967098\n",
            "Batch accuracy:  0.8622448979591837\n",
            "Batch loss:  0.265341\n",
            "Batch accuracy:  0.8590909090909091\n",
            "Batch loss:  0.29541266\n",
            "Batch accuracy:  0.865546218487395\n",
            "Batch loss:  0.36501738\n",
            "Batch accuracy:  0.8914728682170543\n",
            "Batch loss:  0.31030557\n",
            "Epoch ended\n",
            "Total loss:  33.49558600783348\n",
            "Average batch loss:  0.30450532734394076\n",
            "Starting epoch:  91\n",
            "Batch accuracy:  0.8821138211382114\n",
            "Batch loss:  0.3122571\n",
            "Batch accuracy:  0.8653846153846154\n",
            "Batch loss:  0.35184675\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.2692133\n",
            "Batch accuracy:  0.7601351351351351\n",
            "Batch loss:  0.49455813\n",
            "Batch accuracy:  0.8796296296296297\n",
            "Batch loss:  0.24170513\n",
            "Batch accuracy:  0.8829787234042553\n",
            "Batch loss:  0.31426966\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.34404805\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.25004673\n",
            "Batch accuracy:  0.8826530612244898\n",
            "Batch loss:  0.28142035\n",
            "Batch accuracy:  0.8590909090909091\n",
            "Batch loss:  0.34146327\n",
            "Batch accuracy:  0.9327731092436975\n",
            "Batch loss:  0.20927295\n",
            "Batch accuracy:  0.8798449612403101\n",
            "Batch loss:  0.3320652\n",
            "Epoch ended\n",
            "Total loss:  34.54753619432449\n",
            "Average batch loss:  0.3140685108574954\n",
            "Starting epoch:  92\n",
            "Batch accuracy:  0.8983739837398373\n",
            "Batch loss:  0.23019046\n",
            "Batch accuracy:  0.8525641025641025\n",
            "Batch loss:  0.4802761\n",
            "Batch accuracy:  0.9256756756756757\n",
            "Batch loss:  0.16443641\n",
            "Batch accuracy:  0.8243243243243243\n",
            "Batch loss:  0.3930198\n",
            "Batch accuracy:  0.9120370370370371\n",
            "Batch loss:  0.2819034\n",
            "Batch accuracy:  0.925531914893617\n",
            "Batch loss:  0.2412727\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.30611032\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.2744879\n",
            "Batch accuracy:  0.8877551020408163\n",
            "Batch loss:  0.3014711\n",
            "Batch accuracy:  0.9\n",
            "Batch loss:  0.28969768\n",
            "Batch accuracy:  0.8445378151260504\n",
            "Batch loss:  0.38802248\n",
            "Batch accuracy:  0.8953488372093024\n",
            "Batch loss:  0.3014986\n",
            "Epoch ended\n",
            "Total loss:  33.064076259732246\n",
            "Average batch loss:  0.30058251145211135\n",
            "Starting epoch:  93\n",
            "Batch accuracy:  0.8983739837398373\n",
            "Batch loss:  0.2748158\n",
            "Batch accuracy:  0.9166666666666666\n",
            "Batch loss:  0.24577092\n",
            "Batch accuracy:  0.8040540540540541\n",
            "Batch loss:  0.4443038\n",
            "Batch accuracy:  0.8851351351351351\n",
            "Batch loss:  0.2843691\n",
            "Batch accuracy:  0.8472222222222222\n",
            "Batch loss:  0.3355967\n",
            "Batch accuracy:  0.9361702127659575\n",
            "Batch loss:  0.20358096\n",
            "Batch accuracy:  0.8705357142857143\n",
            "Batch loss:  0.372705\n",
            "Batch accuracy:  0.8543689320388349\n",
            "Batch loss:  0.28072575\n",
            "Batch accuracy:  0.9030612244897959\n",
            "Batch loss:  0.24554996\n",
            "Batch accuracy:  0.8454545454545455\n",
            "Batch loss:  0.3766873\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.3359314\n",
            "Batch accuracy:  0.8875968992248062\n",
            "Batch loss:  0.3217673\n",
            "Epoch ended\n",
            "Total loss:  33.21756260097027\n",
            "Average batch loss:  0.30197784182700244\n",
            "Starting epoch:  94\n",
            "Batch accuracy:  0.8617886178861789\n",
            "Batch loss:  0.30265474\n",
            "Batch accuracy:  0.8589743589743589\n",
            "Batch loss:  0.36261144\n",
            "Batch accuracy:  0.8986486486486487\n",
            "Batch loss:  0.2368671\n",
            "Batch accuracy:  0.8885135135135135\n",
            "Batch loss:  0.34703785\n",
            "Batch accuracy:  0.9074074074074074\n",
            "Batch loss:  0.26478744\n",
            "Batch accuracy:  0.8776595744680851\n",
            "Batch loss:  0.26560012\n",
            "Batch accuracy:  0.8348214285714286\n",
            "Batch loss:  0.4109713\n",
            "Batch accuracy:  0.8883495145631068\n",
            "Batch loss:  0.31237212\n",
            "Batch accuracy:  0.9285714285714286\n",
            "Batch loss:  0.19461918\n",
            "Batch accuracy:  0.8909090909090909\n",
            "Batch loss:  0.27443147\n",
            "Batch accuracy:  0.8487394957983193\n",
            "Batch loss:  0.33073652\n",
            "Batch accuracy:  0.8372093023255814\n",
            "Batch loss:  0.38203204\n",
            "Epoch ended\n",
            "Total loss:  34.20318555831909\n",
            "Average batch loss:  0.31093805053017354\n",
            "Starting epoch:  95\n",
            "Batch accuracy:  0.8983739837398373\n",
            "Batch loss:  0.26025164\n",
            "Batch accuracy:  0.9166666666666666\n",
            "Batch loss:  0.22965953\n",
            "Batch accuracy:  0.9324324324324325\n",
            "Batch loss:  0.20389786\n",
            "Batch accuracy:  0.8141891891891891\n",
            "Batch loss:  0.5650689\n",
            "Batch accuracy:  0.8935185185185185\n",
            "Batch loss:  0.27225932\n",
            "Batch accuracy:  0.898936170212766\n",
            "Batch loss:  0.2534573\n",
            "Batch accuracy:  0.8482142857142857\n",
            "Batch loss:  0.30791336\n",
            "Batch accuracy:  0.9320388349514563\n",
            "Batch loss:  0.17399126\n",
            "Batch accuracy:  0.8775510204081632\n",
            "Batch loss:  0.26687524\n",
            "Batch accuracy:  0.8909090909090909\n",
            "Batch loss:  0.30702254\n",
            "Batch accuracy:  0.8991596638655462\n",
            "Batch loss:  0.23678647\n",
            "Batch accuracy:  0.8217054263565892\n",
            "Batch loss:  0.37577543\n",
            "Epoch ended\n",
            "Total loss:  33.64486047625542\n",
            "Average batch loss:  0.30586236796595834\n",
            "Starting epoch:  96\n",
            "Batch accuracy:  0.8780487804878049\n",
            "Batch loss:  0.30379733\n",
            "Batch accuracy:  0.7692307692307693\n",
            "Batch loss:  0.35183764\n",
            "Batch accuracy:  0.8648648648648649\n",
            "Batch loss:  0.25842586\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.42920566\n",
            "Batch accuracy:  0.8379629629629629\n",
            "Batch loss:  0.32543543\n",
            "Batch accuracy:  0.8936170212765957\n",
            "Batch loss:  0.25905657\n",
            "Batch accuracy:  0.8035714285714286\n",
            "Batch loss:  0.41644838\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.30278897\n",
            "Batch accuracy:  0.9132653061224489\n",
            "Batch loss:  0.18548478\n",
            "Batch accuracy:  0.7909090909090909\n",
            "Batch loss:  0.5718743\n",
            "Batch accuracy:  0.8949579831932774\n",
            "Batch loss:  0.21546762\n",
            "Batch accuracy:  0.7906976744186046\n",
            "Batch loss:  0.35972217\n",
            "Epoch ended\n",
            "Total loss:  32.908990025520325\n",
            "Average batch loss:  0.2991726365956393\n",
            "Starting epoch:  97\n",
            "Batch accuracy:  0.8821138211382114\n",
            "Batch loss:  0.26655307\n",
            "Batch accuracy:  0.9038461538461539\n",
            "Batch loss:  0.24266918\n",
            "Batch accuracy:  0.918918918918919\n",
            "Batch loss:  0.16340742\n",
            "Batch accuracy:  0.8952702702702703\n",
            "Batch loss:  0.26871505\n",
            "Batch accuracy:  0.8055555555555556\n",
            "Batch loss:  0.446712\n",
            "Batch accuracy:  0.8723404255319149\n",
            "Batch loss:  0.24716432\n",
            "Batch accuracy:  0.8839285714285714\n",
            "Batch loss:  0.3606384\n",
            "Batch accuracy:  0.912621359223301\n",
            "Batch loss:  0.22222503\n",
            "Batch accuracy:  0.8724489795918368\n",
            "Batch loss:  0.3574085\n",
            "Batch accuracy:  0.8863636363636364\n",
            "Batch loss:  0.27186617\n",
            "Batch accuracy:  0.8907563025210085\n",
            "Batch loss:  0.24470718\n",
            "Batch accuracy:  0.8604651162790697\n",
            "Batch loss:  0.33438513\n",
            "Epoch ended\n",
            "Total loss:  32.6158953756094\n",
            "Average batch loss:  0.29650813977826723\n",
            "Starting epoch:  98\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.28308076\n",
            "Batch accuracy:  0.9166666666666666\n",
            "Batch loss:  0.2411961\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.2790967\n",
            "Batch accuracy:  0.8918918918918919\n",
            "Batch loss:  0.26816443\n",
            "Batch accuracy:  0.8657407407407407\n",
            "Batch loss:  0.35454252\n",
            "Batch accuracy:  0.8936170212765957\n",
            "Batch loss:  0.2406348\n",
            "Batch accuracy:  0.8660714285714286\n",
            "Batch loss:  0.3225755\n",
            "Batch accuracy:  0.7961165048543689\n",
            "Batch loss:  0.6146715\n",
            "Batch accuracy:  0.8367346938775511\n",
            "Batch loss:  0.32276702\n",
            "Batch accuracy:  0.8545454545454545\n",
            "Batch loss:  0.36716232\n",
            "Batch accuracy:  0.8823529411764706\n",
            "Batch loss:  0.41268232\n",
            "Batch accuracy:  0.875968992248062\n",
            "Batch loss:  0.31836128\n",
            "Epoch ended\n",
            "Total loss:  33.83159416913986\n",
            "Average batch loss:  0.3075599469921806\n",
            "Starting epoch:  99\n",
            "Batch accuracy:  0.8577235772357723\n",
            "Batch loss:  0.30870298\n",
            "Batch accuracy:  0.9166666666666666\n",
            "Batch loss:  0.21467781\n",
            "Batch accuracy:  0.9797297297297297\n",
            "Batch loss:  0.09656203\n",
            "Batch accuracy:  0.831081081081081\n",
            "Batch loss:  0.40479222\n",
            "Batch accuracy:  0.8935185185185185\n",
            "Batch loss:  0.32436773\n",
            "Batch accuracy:  0.7978723404255319\n",
            "Batch loss:  0.35231423\n",
            "Batch accuracy:  0.8392857142857143\n",
            "Batch loss:  0.30229932\n",
            "Batch accuracy:  0.883495145631068\n",
            "Batch loss:  0.39115047\n",
            "Batch accuracy:  0.8571428571428571\n",
            "Batch loss:  0.32904232\n",
            "Batch accuracy:  0.8318181818181818\n",
            "Batch loss:  0.3692088\n",
            "Batch accuracy:  0.907563025210084\n",
            "Batch loss:  0.25701067\n",
            "Batch accuracy:  0.8682170542635659\n",
            "Batch loss:  0.28220233\n",
            "Epoch ended\n",
            "Total loss:  32.76664336025715\n",
            "Average batch loss:  0.2978785760023377\n",
            "Starting epoch:  100\n",
            "Batch accuracy:  0.9146341463414634\n",
            "Batch loss:  0.17107125\n",
            "Batch accuracy:  0.8333333333333334\n",
            "Batch loss:  0.38236165\n",
            "Batch accuracy:  0.9054054054054054\n",
            "Batch loss:  0.22242023\n",
            "Batch accuracy:  0.875\n",
            "Batch loss:  0.28510058\n",
            "Batch accuracy:  0.9074074074074074\n",
            "Batch loss:  0.34967104\n",
            "Batch accuracy:  0.8829787234042553\n",
            "Batch loss:  0.24258943\n",
            "Batch accuracy:  0.8883928571428571\n",
            "Batch loss:  0.23445371\n",
            "Batch accuracy:  0.8932038834951457\n",
            "Batch loss:  0.26057324\n",
            "Batch accuracy:  0.8673469387755102\n",
            "Batch loss:  0.30486986\n",
            "Batch accuracy:  0.8636363636363636\n",
            "Batch loss:  0.29278868\n",
            "Batch accuracy:  0.8403361344537815\n",
            "Batch loss:  0.30439386\n",
            "Batch accuracy:  0.8604651162790697\n",
            "Batch loss:  0.3100639\n",
            "Epoch ended\n",
            "Total loss:  33.42079247534275\n",
            "Average batch loss:  0.30382538613947957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05bkEh7iWVIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38599f07-01ac-49c5-8f70-ab371bad4ba4"
      },
      "source": [
        "test_n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RAZhvwPJIM0A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "540463a4-6af6-4b30-c324-bf7a335496bf"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Session() as sess:\n",
        "    model = Model()\n",
        "#     sess.run(model.init)\n",
        "    output = []\n",
        "    labels = []\n",
        "    model.saver.restore(sess,\"../home/Model2/model.ckpt\")\n",
        "    for i in range(train_n,train_data_n,batch_n):\n",
        "        p1 = train_df['p1'][i:i+batch_n]\n",
        "        p2 = train_df['p2'][i:i+batch_n]\n",
        "        test_data = generate_batch(p1,p2)\n",
        "        X1,X2,label = process_batch(test_data)\n",
        "        l2 = sess.run([model.l1], feed_dict = {model.keep_prob: 1, model.X1 : X2})[0]\n",
        "        out = model.find_output(sess,X1,l2,1)\n",
        "        output.extend(out)\n",
        "        labels.extend(label)\n",
        "    accuracy = np.mean(np.equal(labels,np.round(output)))\n",
        "    print(\"Testing Accuracy: \", accuracy)\n",
        "    "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1000)\n",
            "INFO:tensorflow:Restoring parameters from ../home/Model2/model.ckpt\n",
            "Testing Accuracy:  0.5543478260869565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2susL7ooIM0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d12e7d04-ddde-412a-b1fe-9f12f0b59f64"
      },
      "source": [
        "#submission data\n",
        "sub_data = pd.read_csv('sample_submission.csv')\n",
        "sub_data_n = len(sub_data)\n",
        "print(sub_data_n)\n",
        "sub_data.head()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_pair</th>\n",
              "      <th>is_related</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>face05508.jpg-face01210.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>face05750.jpg-face00898.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>face05820.jpg-face03938.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>face02104.jpg-face01172.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>face02428.jpg-face05611.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      img_pair  is_related\n",
              "0  face05508.jpg-face01210.jpg           0\n",
              "1  face05750.jpg-face00898.jpg           0\n",
              "2  face05820.jpg-face03938.jpg           0\n",
              "3  face02104.jpg-face01172.jpg           0\n",
              "4  face02428.jpg-face05611.jpg           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fAAaaRmlIM0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_sub_data(files):\n",
        "    X1 = []\n",
        "    X2 = []\n",
        "#     files = sub_data['img_pair']\n",
        "    for entry in files:\n",
        "#         print(entry)\n",
        "        img1,img2 = entry.split('-')\n",
        "        path = ''\n",
        "        x1 = np.array(Image.open(path + img1))\n",
        "        x2 = np.array(Image.open(path + img2))\n",
        "        X1.append(process_image(x1))\n",
        "        X2.append(process_image(x2))\n",
        "#     print('Sizes: ',len(X1), \", \",len(X2))\n",
        "    return np.array(X1),np.array(X2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E8TnG8zKIM0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8854a2ad-9e51-4372-b45c-3c53e9361a76"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Session() as sess:\n",
        "    model = Model()\n",
        "    model.saver.restore(sess,'../home/Model1/model.ckpt')\n",
        "    output = []\n",
        "    files = sub_data['img_pair']\n",
        "    for i in range(0,sub_data_n,10):\n",
        "        X1,X2 = process_sub_data(files[i:i+10])\n",
        "        l2 = sess.run([model.l1],feed_dict = {model2.keep_prob: 1, model.X1 : X2})[0]\n",
        "        out = model.find_output(sess,X1,l2,1)\n",
        "        output.extend(out)\n",
        "    print(len(output))\n",
        "    sub_data['is_related'] = np.round(np.array(output))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1000)\n",
            "INFO:tensorflow:Restoring parameters from ../home/Model1/model.ckpt\n",
            "5310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mRu7I2k_IM04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "30c4f8e0-d864-4de8-d0b5-a0ecf0331afe"
      },
      "source": [
        "sub_data.tail()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_pair</th>\n",
              "      <th>is_related</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5305</th>\n",
              "      <td>face99998.jpg-face99993.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5306</th>\n",
              "      <td>face99997.jpg-face99996.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5307</th>\n",
              "      <td>face99997.jpg-face99995.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5308</th>\n",
              "      <td>face99997.jpg-face99994.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5309</th>\n",
              "      <td>face99997.jpg-face99993.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         img_pair  is_related\n",
              "5305  face99998.jpg-face99993.jpg         0.0\n",
              "5306  face99997.jpg-face99996.jpg         0.0\n",
              "5307  face99997.jpg-face99995.jpg         0.0\n",
              "5308  face99997.jpg-face99994.jpg         0.0\n",
              "5309  face99997.jpg-face99993.jpg         0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ctkuYZPjIM1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_data.to_csv('../home/submission.csv',index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HysgmZ5pIM1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0d8f6834-49e7-499b-ab1b-ace4da87bb2c"
      },
      "source": [
        "df = pd.read_csv('../home/submission.csv')\n",
        "df.tail()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_pair</th>\n",
              "      <th>is_related</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5305</th>\n",
              "      <td>face99998.jpg-face99993.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5306</th>\n",
              "      <td>face99997.jpg-face99996.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5307</th>\n",
              "      <td>face99997.jpg-face99995.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5308</th>\n",
              "      <td>face99997.jpg-face99994.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5309</th>\n",
              "      <td>face99997.jpg-face99993.jpg</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         img_pair  is_related\n",
              "5305  face99998.jpg-face99993.jpg         0.0\n",
              "5306  face99997.jpg-face99996.jpg         0.0\n",
              "5307  face99997.jpg-face99995.jpg         0.0\n",
              "5308  face99997.jpg-face99994.jpg         0.0\n",
              "5309  face99997.jpg-face99993.jpg         0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEOHnZX-H-SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1a18300d-e742-4efd-e3a2-0335e7030c26"
      },
      "source": [
        "files = ['../home/submission.csv']\n",
        "files.extend(glob.glob('../home/Model?/*'))\n",
        "print(files)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['../home/submission.csv', '../home/Model2/model.ckpt.meta', '../home/Model2/model.ckpt.index', '../home/Model2/model.ckpt.data-00000-of-00001', '../home/Model2/checkpoint', '../home/Model1/model.ckpt.meta', '../home/Model1/model.ckpt.index', '../home/Model1/model.ckpt.data-00000-of-00001', '../home/Model1/checkpoint']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YpUNWRaJIM1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_folder = train_df['p1'][0]\n",
        "image = np.array([np.array(Image.open(f)) for f in glob.glob(\"../input/train/\" + image_folder + \"/*.jpg\", recursive = True)])\n",
        "image.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i8ZwKerOIM11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t_TDFp-CIM2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_img = np.mean(image,axis = 3)/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BIpMsFSUIM2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(normalized_img[1,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0YmiS7bIM2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flattened_img = normalized_img.reshape(-1,224*224)\n",
        "print(flattened_img.shape)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(flattened_img)\n",
        "pca"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MWS_yzSMIM3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum(pca.explained_variance_ratio_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lRG9xlq0IM3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "components = pca.components_\n",
        "max_val = np.max( -1 * components,axis = 1)\n",
        "max_mat = np.tile(np.array([max_val]),(components.shape[1],1)).T\n",
        "components = (components + max_mat) / max_mat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "j2LwfH0UIM32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,normalized_img.shape[0],figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(normalized_img[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NSq2DhPzIM4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,components.shape[0],figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(components[i].reshape(224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kOw3ZSEqIM4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_eigen_face(image_name):\n",
        "    image = np.array([np.array(Image.open(f)) for f in glob.glob(\"../input/train/\" + image_name + \"/*.jpg\", recursive = True)])\n",
        "    print(image.shape)\n",
        "    normalized_img = np.mean(image,axis = 3)/255 \n",
        "    print(normalized_img.shape)\n",
        "    flattened_img = normalized_img.reshape(-1,224*224) + 0.0001\n",
        "    pca = PCA()\n",
        "    pca.fit(flattened_img)\n",
        "    components = pca.components_\n",
        "    max_val = np.max( -1 * components,axis = 1)\n",
        "    max_mat = np.tile(np.array([max_val]),(components.shape[1],1)).T\n",
        "    components = (components + max_mat) / max_mat\n",
        "    components = components.reshape(-1,224,224)\n",
        "    return normalized_img, components"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yIWCKGK1IM4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_path = train_df['p1'][241]\n",
        "norm,comp = find_eigen_face(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qN5gEAYDIM4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,min(norm.shape[0],8),figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(norm[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rluUmYqyIM47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,min(norm.shape[0],8),figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(comp[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qS7w9HnkIM5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_path = train_df['p2'][241]\n",
        "print(image_path)\n",
        "norm,comp = find_eigen_face(image_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IYQYIHRYIM57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,min(norm.shape[0],8),figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(norm[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zMFaXXFyIM6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,min(norm.shape[0],8),figsize = (40,40))\n",
        "for i,axes in enumerate(ax.reshape(-1)):\n",
        "    axes.imshow(comp[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4kGREgt5IM6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 23"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7zKFFs-GIM6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgp = train_df['p1'][i]\n",
        "img = [np.array(Image.open(f)) for f in glob.glob('../input/train/' + imgp + '/*.jpg', recursive = True)]\n",
        "img1 = img[1]\n",
        "print(img1.shape)\n",
        "img1 = np.mean(img1,axis = 2)/255\n",
        "image = img1 - np.roll(img1,1,axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8RK049kNIM6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Biz89AKdIM7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgp = train_df['p2'][i]\n",
        "img = [np.array(Image.open(f)) for f in glob.glob('../input/train/' + imgp + '/*.jpg', recursive = True)]\n",
        "img1 = img[1]\n",
        "print(img1.shape)\n",
        "img1 = np.mean(img1,axis = 2)/255\n",
        "image = img1 - np.roll(img1,1,axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NyuLLkOqIM7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}